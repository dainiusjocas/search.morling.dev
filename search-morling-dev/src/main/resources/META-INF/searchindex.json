[{"content":"","id":0,"section":"blog","summary":"","tags":null,"title":"Blogs","uri":"http://localhost:1313/blog/","year":"2020"},{"content":"Ahead-of-time compilation (AOT) is the big topic in the Java ecosystem lately: by compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage. Spearheaded by the GraalVM project, Project Leyden promises to standardize AOT in a future version of the Java platform.\n This makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions, in particular when it comes to faster start-up times. Besides a range of improvements related to class loading, linking and bytecode verification, substantial work has been done around class data sharing (CDS). Faster start-ups are beneficial in many ways: shorter turnaround times during development, quicker time-to-first-response for users in coldstart scenarios, cost savings when billed by CPU time in the cloud.\n With CDS, class metadata is persisted in an archive file, which during subsequent application starts is mapped into memory. This is faster than loading the actual class files, resulting in reduced start-up times. When starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\n Originally a partially commercial feature of the Oracle JDK, CDS was completely open-sourced in JDK 10 and got incrementally improved since then in a series of Java improvement proposals:\n   JEP 310, Application Class-Data Sharing (AppCDS), in JDK 10: \"To improve startup and footprint, extend the existing [CDS] feature to allow application classes to be placed in the shared archive\"\n  JEP 341, Default CDS Archives, in JDK 12: \"Enhance the JDK build process to generate a class data-sharing (CDS) archive, using the default class list, on 64-bit platforms\"\n  JEP 350, Dynamic CDS Archives, in JDK 13: \"Extend application class-data sharing to allow the dynamic archiving of classes at the end of Java application execution. The archived classes will include all loaded application classes and library classes that are not present in the default, base-layer CDS archive\"\n   In the remainder of this blog post we\u0026#8217;ll discuss how to automatically create AppCDS archives as part of your (Maven) project build, based on the improvements made with JEP 350. I.e. Java 13 or later is a prerequisite for this. To learn more about using CDS with the current LTS release JDK 11 and about CDS in general, refer to the excellent blog post on everything CDS by Nicolai Parlog.\n Manually Creating CDS Archives At first let\u0026#8217;s see what\u0026#8217;s needed to manually create and use an AppCDS archive (note I\u0026#8217;m going to use \"AppCDS\" and \"CDS\" somewhat interchangeably for the sake of brevity). Subsequently, we\u0026#8217;ll discuss how the task can be automated in a Maven project build.\n To have an example to work with which goes beyond a plain \"Hello World\", I\u0026#8217;ve created a small web application for managing personal to-dos, using the Quarkus stack. If you\u0026#8217;d like to follow along, clone the repo and build the project:\n git clone git@github.com:gunnarmorling/quarkus-cds.git cd quarkus-cds mvn clean verify -DskipTests=true   The application uses a Postgres database for persisting the to-dos; fire it up via Docker:\n cd compose docker run -d -p 5432:5432 --name pgdemodb \\ -v $(pwd)/init.sql:/docker-entrypoint-initdb.d/init.sql \\ -e POSTGRES_USER=todouser \\ -e POSTGRES_PASSWORD=todopw \\ -e POSTGRES_DB=tododb postgres:11   The next step is to run the application and create the CDS archive file. Do so by passing the -XX:ArchiveClassesAtExit option:\n java -XX:ArchiveClassesAtExit=target/app-cds.jsa \\ (1) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 Triggers creation of a CDS archive at the given location upon application shutdown    Only loaded classes will be added to the archive. As classloading on the JVM happens lazily, you must invoke some functionality in your application in order to cause all the relevant classes to be loaded. For that to happen, open the application\u0026#8217;s API endpoint in a browser or invoke it via curl, httpie or similar:\n http localhost:8080/api   Stop the application by hitting Ctrl+C. This will create the CDS archive under target/app-cds.jsa. In our case it should have a size of about 41 MB. Also observe the log messages about classes which were skipped from archiving:\n ... [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$MH+0x0000000800bd0c40: Hidden or Unsafe anonymous class [190.220s][warning][cds] Skipping java/lang/invoke/LambdaForm$DMH+0x0000000800fdc840: Hidden or Unsafe anonymous class [190.220s][warning][cds] Pre JDK 6 class not supported by CDS: 46.0 antlr/TokenStreamIOException ...   Mostly this is about hidden or anonymous classes which cannot be archived; there\u0026#8217;s not so much you can do about that (apart from using less Lambda expressions perhaps\u0026#8230;\u0026#8203;).\n The hint on old classfile versions is more actionable: only classes using classfile format 50 (= JDK 1.6) or newer are supported by CDS. In the case at hand, the classes from Antlr 2.7.7 are using classfile format 46 (which was introduced in Java 1.2) and thus cannot be added to the CDS archive. Note this also applies to any subclasses, even if they themselves use a newer classfile format version.\n It\u0026#8217;s thus a good idea to check whether you can upgrade to newer versions of your dependencies, as this may result in more classes becoming available for CDS, resulting in better start-up times in turn.\n   Using the CDS Archive Now let\u0026#8217;s run the application again, this time using the previously created CDS archive:\n java -XX:SharedArchiveFile=target/app-cds.jsa \\ (1) -Xlog:class+load:file=target/classload.log \\ (2) -Xshare:on \\ (3) -jar target/todo-manager-1.0.0-SNAPSHOT-runner.jar     1 The path to the CDS archive   2 classloading logging allows to verify whether the CDS archive gets applied as expected   3 While class data sharing is enabled by default on JDK 12 and newer, explicitely enforcing it will ensure an error is raised if something is wrong, e.g. a mismatch of Java versions between building and using the archive    When examining the classload.log file, you should see how most class metadata is obtained from the CDS archive (\"source: shared object file\"), while some classes such as the ancient Antlr classes are loaded just as usual from the corresponding JAR:\n [0.016s][info][class,load] java.lang.Object source: shared objects file [0.016s][info][class,load] java.io.Serializable source: shared objects file [0.016s][info][class,load] java.lang.Comparable source: shared objects file [0.016s][info][class,load] java.lang.CharSequence source: shared objects file ... [2.555s][info][class,load] antlr.Parser source: file:/.../antlr.antlr-2.7.7.jar ...   Note it is vital that the exact same Java version is used as when creating the archive, otherwise an error will be raised. Unfortunately, this also means that AppCDS archives cannot be built cross-platform. This would be very useful, e.g. when building a Java application on macOS or Windows, which should be packaged in a Linux container. If you are aware of a way for doing so, please let me know in the comments below.\n     CDS and the Java Module System Beginning with Java 11, not only classes from the classpath can be added to CDS archives, but also classes from the module path of a modularized Java application. One important detail to consider there is that the --upgrade-module-path and --patch-module options will cause CDS to be disabled or disallowed (with -Xshare:on) is specified. This is to avoid a mismatch of class metadata in the CDS archive and classes brought in by a newer module version.\n       Creating CDS Archives in Your Maven Build Manually creating a CDS archive is not very efficient nor reliable, so let\u0026#8217;s see how the task can be automated as part of your project build. The following shows the required configuration when using Apache Maven, but of course the same approach could be implemented with Gradle or any other build system.\n The basic idea is the follow the same steps as before, but executed as part of the Maven build:\n  start up the application with the -XX:ArchiveClassesAtExit option\n  invoke some application functionality to initiate the loading of all relevant classes\n  stop the application\n       It might appear as a compelling idea to produce the CDS archive as part of regular test execution, e.g. via JUnit. This will not work though, as the classpath at the time of using the CDS archive must be not miss any entries from the classpath at the time of creating it. As during test execution all the test-scoped dependencies will be part of the classpath, any CDS archive created that way couldn\u0026#8217;t be used when running the application later on without those test dependencies.\n     Steps 1. and 3. can be automated with help of the Process-Exec Maven plug-in, binding it to the pre-integration-test and post-integration-test build phases, respectively. While I was thinking of using the more widely known Exec plug-in initially, this turned out to not be viable as there\u0026#8217;s no way for stopping any forked process in a later build phase.\n Here\u0026#8217;s the relevant configuration:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;com.bazaarvoice.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;process-exec-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.9\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; (1) \u0026lt;id\u0026gt;app-cds-creation\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;pre-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;start\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;name\u0026gt;todo-manager\u0026lt;/name\u0026gt; \u0026lt;healthcheckUrl\u0026gt;http://localhost:8080/\u0026lt;/healthcheckUrl\u0026gt; (2) \u0026lt;arguments\u0026gt; \u0026lt;argument\u0026gt;java\u0026lt;/argument\u0026gt; (3) \u0026lt;argument\u0026gt;-XX:ArchiveClassesAtExit=app-cds.jsa\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt;-jar\u0026lt;/argument\u0026gt; \u0026lt;argument\u0026gt; ${project.build.directory}/${project.artifactId}-${project.version}-runner.jar \u0026lt;/argument\u0026gt; \u0026lt;/arguments\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;execution\u0026gt; (4) \u0026lt;id\u0026gt;stop-all\u0026lt;/id\u0026gt; \u0026lt;phase\u0026gt;post-integration-test\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;stop-all\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...     1 Start up the application in the pre-integration-test build phase   2 The health-check URL is used to await application start-up before proceeding with the next build phase   3 Assemble the java invocation   4 Stop the application in the post-integration-test build phase    What remains to be done is the automation of step 2, the invocation of the required application logic so to trigger the loading of all relevant classes. This can be done with help of the Maven Surefire plug-in. A simple \"integration test\" via REST Assured does the trick:\n public class ExampleResourceAppCds { @Test public void getAll() { given() .when() .get(\"/api\") .then() .statusCode(200); } }   We just need to configure a specific execution of the plug-in, which only picks up any test classes whose names end with *AppCds.java, so to keep them apart from actual integration tests:\n ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-failsafe-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M4\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;integration-test\u0026lt;/goal\u0026gt; \u0026lt;goal\u0026gt;verify\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*AppCds.java\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...   And that\u0026#8217;s all we need; when now building the project via mvn clean verify, a CDS archive will be created at target/app-cds.jsa. You can find the complete example project and steps for building/running it on GitHub.\n   What Do You Gain? Creating a CDS archive is nice, but is it also worth the effort? In order to answer this question, I\u0026#8217;ve done some measurements of the \"time-to-first-response\" metric, following the Quarkus guide on measuring performance. I.e. instead of awaiting some rather meaningless \"start-up complete\" status, which could arbitrarily be tweaked by means of lazy initialization, this measures the time until the application is actually ready to handle the first incoming request after start-up.\n I\u0026#8217;ve done measurements on OpenJDK 1.8.0_252 (AdoptOpenJDK build), OpenJDK 14.0.1 (upstream build, without and with AppCDS), and OpenJDK 15-ea-b26 (upstream build, with AppCDS). Please see the README file of the example repo for the exact steps.\n Here are the numbers, averaged over ten runs each:\n   Update, June 12th: I had originally classload logging enabled for the OpenJDK 14 AppCDS runs, which added an unneccessary overhead (thanks a lot to Claes Redestad for pointing this out!). The numbers and chart have been updated accordingly. I\u0026#8217;ve also added numbers for OpenJDK 15-ea.\n Time-to-first-response values are 2s 267ms, 2s 162ms, 1s 669ms 1s 483ms, and 1s 279ms. I.e. on my machine (2014 MacBook Pro), with this specific workload, there\u0026#8217;s an improvement of ~100ms just by upgrading to the current JDK, and of another ~500ms ~700ms by using AppCDS.\n With OpenJDK 15 things will further improve. The latest EA build at the time of writing (b26) shortens time-to-first-response by another ~200ms. The upcoming EA build 27 should bring another improvement, as Lambda proxy classes will be added to AppCDS archives then.\n That all is definitely a nice improvement, in particular as we get it essentially for free, without any changes to the actual application itself. You should contrast this with the additional size of the application distribution, though. E.g. when obtaining the application as a container image from a remote container registry, downloading the additional ~40 MB might take longer than the time saved during application start-up. Typically, this will only affect the first start-up of on a particular node, though, after which the image will be cached locally.\n As always when it comes to any kinds of performance numbers, please take these numbers with a grain of salt, do your own measurements, using your own applications and in your own environment.\n     Addressing Different Workload Profiles If your application supports different \"work modes\", e.g. \"online\" and \"batch\", which work with a largely differing set of classes, you also might consider to create different CDS archives for the specific workloads. This might give you a good balance between additional size and realized improvements of start-up times, when for instance dealing with at large monolithic application instead of more fine-grained microservices.\n       Wrap-Up AppCDS provides Java developers with a useful tool for reducing start-up times of their applications, without requiring any code changes. For the example discussed, we could observe an improvement of the time-to-first-response metric by about 30% when running with OpenJDK 14. Other users reported even bigger improvements.\n We didn\u0026#8217;t discuss any potential memory improvements due to CDS when sharing class metadata between multiple JVMs on one host. In containerized server applications, with each JVM being packaged in its own container image, this won\u0026#8217;t play a role. It could make a difference on desktop systems, though. For instance multiple instances of the Java language server, as leveraged by VSCode and other editors, could benefit from that.\n That all being said, when raw start-up time is your primary concern, e.g. in a serverless or Function-based setting, you should look at AOT compilation with GraalVM (or Project Leyden in the future). This will bring down start-up times to a completely different level; for example the todo manager application would return a first response within a few 10s of milliseconds when executed as a native image via GraalVM.\n But AOT is not always an option, nor does it always make sense: the JVM may offer a better latency than native binaries, external dependencies migh not be ready for usage in AOT-compiled native images yet, or you simply might want to be able to benefit from all the JVM goodness, like familiar debugging tools, the JDK Flight Recorder, or JMX. In that case, CDS can give you a nice start-up time improvement, solely by means of adding a few steps to your build process.\n Besides class data sharing in OpenJDK, there are some other related techniques for improving start-up times which are worth exploring:\n   Eclipse OpenJ9 has its own implementation of class data sharing\n  Alibaba\u0026#8217;s Dragonwell distribution of the OpenJDK comes with JWarmUp, a tool for speeding up initial JIT compilations\n   To learn more about AppCDS, a long yet insightful post is this one by Vladimir Plizga. Volker Simonis did another interesting write-up. Also take a look at the CDS documentation in the reference docs of the java command.\n Lastly, the Quarkus team is working on out-of-the-box support for CDS archives. This could fully automate the creation of an archive for all required classes without any further configuration, making it even easier to benefit from the start-up time improvements promised by CDS.\n  ","id":1,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAhead-of-time compilation (AOT) is \u003cem\u003ethe\u003c/em\u003e big topic in the Java ecosystem lately:\nby compiling Java code to native binaries, developers and users benefit from vastly improved start-up times and reduced memory usage.\nSpearheaded by the \u003ca href=\"https://www.graalvm.org/\"\u003eGraalVM\u003c/a\u003e project,\n\u003ca href=\"https://mail.openjdk.java.net/pipermail/discuss/2020-April/005429.html\"\u003eProject Leyden\u003c/a\u003e promises to standardize AOT in a future version of the Java platform.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThis makes it easy to miss out on significant performance improvements which have been made on the JVM in recent Java versions,\nin particular when it comes to \u003ca href=\"https://cl4es.github.io/2019/11/20/OpenJDK-Startup-Update.html\"\u003efaster start-up times\u003c/a\u003e.\nBesides a range of improvements related to class loading, linking and bytecode verification,\nsubstantial work has been done around \u003ca href=\"https://docs.oracle.com/en/java/javase/14/vm/class-data-sharing.html\"\u003eclass data sharing\u003c/a\u003e (CDS).\nFaster start-ups are beneficial in many ways:\nshorter turnaround times during development,\nquicker time-to-first-response for users in coldstart scenarios,\ncost savings when billed by CPU time in the cloud.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWith CDS, class metadata is persisted in an archive file,\nwhich during subsequent application starts is mapped into memory.\nThis is faster than loading the actual class files, resulting in reduced start-up times.\nWhen starting multiple JVM processes on the same host, read-only archives of class metadata can also be shared between the VMs, so that less memory is consumed overall.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building Class Data Sharing Archives with Apache Maven","uri":"http://localhost:1313/blog/building-class-data-sharing-archives-with-apache-maven/","year":"2020"},{"content":"layrrry part 3\n reacting to layer changes jfr events class unloading immutable on server side\n The AOT (ahead-of-time) compilation bug has caught Java: great progress is being made in the GraalVM project, and Project Leyden promises to standardize AOT in a future version of the Java platform.\n Manual Creation   Creating AppCDS Archives in your Maven Build appcds Pre JDK 6 class not supported by CDS:\n profile\n https://medium.com/@toparvion/appcds-for-spring-boot-applications-first-contact-6216db6a4194\n memory?\n Do you remember Angus \"Mac\" MacGyver? The always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\n The single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\n   How to change the timezone or format of date/time message fields?\n  How to change the topic a specific message gets sent to?\n  How to filter out specific records?\n   SMTs can be the answer to these and many other questions that come up in the context of Kafka Connect. Applied to source or sink connectors, SMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\n In this post I\u0026#8217;d like to focus on some interesting (hopefully anyways) usages of SMTs. Those use cases are mostly based on my experiences from using Kafka Connect with Debezium, an open-source platform for change data capture (CDC). I also got some great pointers on interesting SMT usages when asking the community about this on Twitter some time ago:\n    I definitely recommend to check out the thread; thanks a lot to all who replied! In order to learn more about SMTs in general, how to configure them etc., refer to the resources given towards the end of this post.\n For each category of use cases, I\u0026#8217;ve also asked our sympathetic TV hero for his opinion on the usefulness of SMTs for the task at hand. You can find his rating at the end of each section, ranging from 📎 (poor fit) to 📎📎📎📎📎 (perfect fit).\n   Format Conversions Probably the most common application of SMTs is format conversion, i.e. adjustments to type, format and representation of data. This may apply to entire messages, or to specific message attributes. Let\u0026#8217;s first look at a few examples for converting individual message attribute formats:\n   Timestamps: Different systems tend to have different assumptions of how timestamps should be typed and formatted. Debezium for instance represents most temporal column types as milli-seconds since epoch. Change event consumers on the other hand might expect such date and time values using Kafka Connect\u0026#8217;s Date type, or as an ISO-8601 formatted string, potentially using a specific timezone\n  Value masking: Sensitive data might have be to masked or truncated, or specific fields should even be removed altogether; the org.apache.kafka.connect.transforms.MaskField and ReplaceField SMTs shipping with Kafka Connect out of the box come in handy for that\n  Numeric types: Similar to timestamps, requirements around the representation of (decimal) numbers may differ between systems; e.g. Kafka Connect\u0026#8217;s Decimal type allows to convey arbitrary-precision decimals, but its binary representation of numbers might not be supported by all sink connectors and consumers\n  Name adjustments: Depending on the chosen serialization formats, specific field names might be unsupported; when working with Apache Avro for instance, field names must not start with a number\n   In all these cases, either existing, ready-made SMTs or bespoke implementations can be used to apply the required attribute type and/or format conversions.\n When using Kafka Connect for integrating legacy services and databases with newly built microservices, such format conversions can play an important role for creating an anti-corruption layer: by using better field names, choosing more suitable data types or by removing unneeded fields, SMTs can help to shield a new service\u0026#8217;s model from the oddities and quirks of the legacy world.\n But SMTs cannot only modify the representation of single fields, also the format and structure of entire messages can be adjusted. E.g. Kafka Connect\u0026#8217;s ExtractField transformation allows to extract a single field from a message and propagate that one. A related SMT is Debezium\u0026#8217;s SMT for change event flattening. It can be used to convert the complex Debezium change event structure with old and new row state, metadata and more, into a flat row representation, which can be consumed by many existing sink connectors.\n SMTs also allow to fine-tune schema namespaces; that can be of interest when working with a schema registry for managing schemas and their versions, and specific schema namespaces should be enforced for the messages on given topics. Two more, very useful examples of SMTs in this category are kafka-connect-transform-xml and kafka-connect-json-schema by Jeremy Custenborder, which will take XML or text and produce a typed Kafka Connect Struct, based on a given XML schema or JSON schema, respectively.\n Lastly, as a special kind of format conversion, SMTs can be used to modify or set the key of Kafka records. This may be desirable if a source connector doesn\u0026#8217;t produce any meaningful key, but one can be extracted from the record value. Also changing the message key can be useful, when considering subsequent stream processing. Choosing matching keys right at the source side e.g. allows for joining multiple topics via Kafka Streams, without the need for re-keying records.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs are the perfect tool for format conversions of Kafka Connect records\n   Ensuring Backwards Compatibility Changes to the schema of Kafka records can potentially be disruptive for consumers. If for instance a record field gets renamed, a consumer must be adapted accordingly, reading the value using the new field name. In case a field gets dropped altogether, consumers must not expect this field any longer.\n Message transformations can help with such transition from one schema version to the next, thus reducing the coupling of the lifecycles of message producers and consumers. In case of a renamed field, an SMT could add the field another time, using the original name. That\u0026#8217;ll allow consumers to continue reading the field using the old name and to be upgraded to use the new name at their own pace. After some time, once all consumers have been adjusted, the SMT can be removed again, only exposing the new field name going forward. Similarly, a field that got removed from a message schema could be re-added, e.g. using some sort of constant placeholder value. In other cases it might be possible to derive the field value from other, still existing fields. Again consumers could then be updated at their own pace to not expect and access that field any longer.\n It should be said though that there are limits for this usage: e.g. when changing the type of a field, things quickly become tricky. One option could be a multi-step approach where at first a separate field with the new type is added, before renaming it again as described above.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎\u0026nbsp;\u0026nbsp; SMTs can primarily help to address basic compatibility concerns around schema evolution\n   Filtering and Routing When applied on the source side, SMTs allow to filter out specific records produced by the connector. They also can be used for controlling the Kafka topic a record gets sent to. That\u0026#8217;s in particular interesting when filtering and routing is based on the actual record contents. In an IoT scenario for instance where Kafka Connect is used to ingest data from some kind of sensors, an SMT might be used to filter out all sensor measurements below a certain threshold, or route measurement events above a threshold to a special topic.\n Debezium provides a range of SMTs for record filtering and routing:\n   The logical topic routing SMT allows to send change events originating from multiple tables to the same Kafka topic, which can be useful when working with partition tables in Postgres, or with data that is sharded into multiple tables\n  The Filter and ContentBasedRouter SMTs let you use script expressions in languages such as Groovy or JavaScript for filtering and routing change events based on their contents; such script-based approach can be an interesting middleground between ease-of-use (no Java code must be compiled and deployed to Kafka Connect) and expressiveness; e.g. here is how the routing SMT could be used with GraalVM\u0026#8217;s JavaScript engine for routing change events from a table with purchase orders to different topics in Kafka, based on the order type:\n... transforms=route transforms.route.type=io.debezium.transforms.ContentBasedRouter transforms.route.topic.regex=.*purchaseorders transforms.route.language=jsr223.graal.js transforms.route.topic.expression= value.after.ordertype == 'B2B' ? 'b2b_orders' : 'b2c_orders' ...     The outbox event router comes in handy when implementing the transactional outbox pattern for data propagation between microservices: it can be used to send events originating from a single outbox table to a specific Kafka topic per aggregate (when thinking of domain driven design) or event type\n   There are also two SMTs for routing purposes in Kafka Connect itself: RegexRouter which allows to re-route records two different topics based on regular expressions, and TimestampRouter for determining topic names based on the record\u0026#8217;s timestamp.\n While routing SMTs usually are applied to source connectors (defining the Kafka topic a record gets sent to), it can also make sense to use them with sink connectors. That\u0026#8217;s the case when a sink connector derives the name of downstream table names, index names or similar from the topic name.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; Message filtering and topic routing\u0026#8201;\u0026#8212;\u0026#8201;no problem for SMTs\n   Tombstone Handling Tombstone records are Kafka records with a null value. They carry special semantics when working with compacted topics: during log compaction, all records with the same key as a tombstone record will be removed from the topic.\n Tombstones will be retained on a topic for a configurable time before compaction happens (controlled via delete.retention.ms topic setting), which means that also Kafka Connect sink connectors need to handle them. Unfortunately though, not all connectors are prepared for records with a null value, typically resulting in NullPointerExceptions and similar. A filtering SMT such as the one above can be used to drop tombstone records in such case.\n But also the exact opposite\u0026#8201;\u0026#8212;\u0026#8201;producing tombstone records\u0026#8201;\u0026#8212;\u0026#8201;can be useful: some sink connectors use tombstone records as the indicator to delete corresponding rows from a downstream datastore. Now when using a CDC connector like Debezium to capture changes from a database where \"soft deletes\" are used (i.e. records are not physically deleted, but a logically deleted flag is set to true when deleting a record), those change events will be exported as update events (which they technically are). A bespoke SMT can be used to translate these update events into tombstone records, triggering the deletion of corresponding records in downstream datastores.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs work well to discard tombstones or convert soft delete events into tombstones. What\u0026#8217;s not possible though is to keep the original event and produce an additional tombstone record at the same time\n   Externalizing Large Payloads Even some advanced enterprise application patterns can be implemented with the help of SMTs, one example being the claim check pattern. This pattern comes in handy in situations like this:\n  A message may contain a set of data items that may be needed later in the message flow, but that are not necessary for all intermediate processing steps. We may not want to carry all this information through each processing step because it may cause performance degradation and makes debugging harder because we carry so much extra data.\n \u0026#8201;\u0026#8212;\u0026#8201;Gregor Hohpe, Bobby Woolf; Enterprise Application Patterns\n   A specific example could again be a CDC connector that captures changes from a database table Users, with a BLOB column that contains the user\u0026#8217;s profile picture (surely not a best practice, still not that uncommon in reality\u0026#8230;\u0026#8203;).\n     Apache Kafka and Large Messages Apache Kafka isn\u0026#8217;t meant for large messages. The maximum message size is 1 MB by default, and while this can be increased, benchmarks are showing best throughput for much smaller messages. Strategies like chunking and externalizing large payloads can thus be vital in order to ensure a satisfying performance.\n     When propagating change data events from that table to Apache Kafka, adding the picture data to each event poses a significant overhead. In particular, if the picture BLOB hasn\u0026#8217;t changed between two events at all.\n Using an SMT, the BLOB data could be externalized to some other storage. On the source side, the SMT could extract the image data from the original record and e.g. write it to a network file system or an Amazon S3 bucket. The corresponding field in the record would be updated so it just contains the unique address of the externalised payload, such as the S3 bucket name and file path:\n   As an optimization, it could be avoided to re-upload unchanged file contents another time by comparing earlier and current hash of the externalized file.\n A corresponding SMT instance applied to sink connectors would retrieve the identifier of the externalized files from the incoming record, obtain the contents from the external storage and put it back into the record before passing it on to the connector.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs can help to externalize payloads, avoiding large Kafka records. Relying on another service increases overall complexity, though\n   Limitations As we\u0026#8217;ve seen, single message transformations can help to address quite a few requirements that commonly come up for users of Kafka Connect. But there are limitations, too; Like MacGyver, who sometimes has to reach for some other tool than his beloved Swiss Army knife, you shouldn\u0026#8217;t think of SMTs as the perfect solution all the time.\n The biggest shortcoming is already hinted at in their name: SMTs only can be used to process single records, one at a time. E.g. you cannot split up a record into multiple ones using an SMT, as they only can return (at most) one record. Also any kind of stateful processing, like aggregating data from multiple records, or correlating records from several topics is off limits for SMTs. For such use cases, you should be looking at stream processing technologies like Kafka Streams and Apache Flink; also integration technologies like Apache Camel can be of great use here.\n One thing to be aware of when working with SMTs is configuration complexity; when using generic, highly configurable SMTs, you might end up with lengthy configuration that\u0026#8217;s hard to grasp and debug. You might be better off implementing a bespoke SMT which is focussing on one particular task, leveraging the full capabilities of the Java programming language.\n     SMT Testing Whether you use ready-made SMTs by means of configuration, or you implement custom SMTs in Java, testing your work is essential.\n While unit tests are a viable option for basic testing of bespoke SMT implementations, integration tests running against Kafka Connect connectors are recommended for testing SMT configurations. That way you\u0026#8217;ll be sure that the SMT can process actual messages and it has been configured the way you intended to.\n Testcontainers and the Debezium support for Testcontainers are a great foundation for setting up all the required components such as Apache Kafka, Kafka Connect, connectors and the SMTs to test.\n     A specific feature I wished for every now and then is the ability to apply SMTs only to a specific sub-set of the topics created or consumed by a connector. In particular if connectors create different kinds of topics (like an actual data topic and another one with with metadata), it can be desirable to apply SMTs only to the topics of one group but not the other. This requirement is captured in KIP-585 (\"Filter and Conditional SMTs\"), please join the discussion on that one if you got requirements or feedback related to that.\n   Learning More There are several great presentations and blog posts out there which describe in depth what SMTs are, how you can implement your own one, how they are configured etc.\n Here are a few resources I found particularly helpful:\n   KIP-66: The original KIP (Kafka Improvement Proposal) that introduced SMTs\n  Singe Message Transforms are not the Transformations You\u0026#8217;re Looking For: A great overview on SMTs, their capabilities as well as limitations, by Ewen Cheslack-Postava\n  A hands-on experience with Kafka Connect SMTs: In-depth blog post on SMT use cases, things to be aware of and more, by Gian D\u0026#8217;Uia\n   Now, considering this wide range of use cases for SMTs, would MacGyver like and use them for implementing various tasks around Kafka Connect? I would certainly think so. But as always, the right tool for the job must be chosen: sometimes an SMT may be a great fit, another time a more flexible (and complex) stream processing solution might be preferable.\n Just as MacGyver, you got to make a call when to use your Swiss Army knife, duct tape or a paper clip.\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":2,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003elayrrry part 3\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003ereacting to layer changes\njfr events\nclass unloading\nimmutable on server side\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe AOT (ahead-of-time) compilation bug has caught Java:\ngreat progress is being made in the GraalVM project,\nand Project Leyden promises to standardize AOT in a future version of the Java platform.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"sect1\"\u003e\n\u003ch2 id=\"_manual_creation\"\u003eManual Creation\u003c/h2\u003e\n\u003cdiv class=\"sectionbody\"\u003e\n\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"sect1\"\u003e\n\u003ch2 id=\"_creating_appcds_archives_in_your_maven_build\"\u003eCreating AppCDS Archives in your Maven Build\u003c/h2\u003e\n\u003cdiv class=\"sectionbody\"\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eappcds Pre JDK 6 class not supported by CDS:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eprofile\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003e\u003ca href=\"https://medium.com/@toparvion/appcds-for-spring-boot-applications-first-contact-6216db6a4194\" class=\"bare\"\u003ehttps://medium.com/@toparvion/appcds-for-spring-boot-applications-first-contact-6216db6a4194\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003ememory?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDo you remember Angus \"Mac\" MacGyver?\nThe always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the timezone or format of date/time message fields?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the topic a specific message gets sent to?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to filter out specific records?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSMTs can be the answer to these and many other questions that come up in the context of Kafka Connect.\nApplied to source or sink connectors,\nSMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Building AppCDS Archives with Maven","uri":"http://localhost:1313/blog/layrry_part_3/","year":"2020"},{"content":"Do you remember Angus \"Mac\" MacGyver? The always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\n The single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\n   How to change the timezone or format of date/time message fields?\n  How to change the topic a specific message gets sent to?\n  How to filter out specific records?\n   SMTs can be the answer to these and many other questions that come up in the context of Kafka Connect. Applied to source or sink connectors, SMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\n In this post I\u0026#8217;d like to focus on some interesting (hopefully anyways) usages of SMTs. Those use cases are mostly based on my experiences from using Kafka Connect with Debezium, an open-source platform for change data capture (CDC). I also got some great pointers on interesting SMT usages when asking the community about this on Twitter some time ago:\n    I definitely recommend to check out the thread; thanks a lot to all who replied! In order to learn more about SMTs in general, how to configure them etc., refer to the resources given towards the end of this post.\n For each category of use cases, I\u0026#8217;ve also asked our sympathetic TV hero for his opinion on the usefulness of SMTs for the task at hand. You can find his rating at the end of each section, ranging from 📎 (poor fit) to 📎📎📎📎📎 (perfect fit).\n Format Conversions Probably the most common application of SMTs is format conversion, i.e. adjustments to type, format and representation of data. This may apply to entire messages, or to specific message attributes. Let\u0026#8217;s first look at a few examples for converting individual message attribute formats:\n   Timestamps: Different systems tend to have different assumptions of how timestamps should be typed and formatted. Debezium for instance represents most temporal column types as milli-seconds since epoch. Change event consumers on the other hand might expect such date and time values using Kafka Connect\u0026#8217;s Date type, or as an ISO-8601 formatted string, potentially using a specific timezone\n  Value masking: Sensitive data might have be to masked or truncated, or specific fields should even be removed altogether; the org.apache.kafka.connect.transforms.MaskField and ReplaceField SMTs shipping with Kafka Connect out of the box come in handy for that\n  Numeric types: Similar to timestamps, requirements around the representation of (decimal) numbers may differ between systems; e.g. Kafka Connect\u0026#8217;s Decimal type allows to convey arbitrary-precision decimals, but its binary representation of numbers might not be supported by all sink connectors and consumers\n  Name adjustments: Depending on the chosen serialization formats, specific field names might be unsupported; when working with Apache Avro for instance, field names must not start with a number\n   In all these cases, either existing, ready-made SMTs or bespoke implementations can be used to apply the required attribute type and/or format conversions.\n When using Kafka Connect for integrating legacy services and databases with newly built microservices, such format conversions can play an important role for creating an anti-corruption layer: by using better field names, choosing more suitable data types or by removing unneeded fields, SMTs can help to shield a new service\u0026#8217;s model from the oddities and quirks of the legacy world.\n But SMTs cannot only modify the representation of single fields, also the format and structure of entire messages can be adjusted. E.g. Kafka Connect\u0026#8217;s ExtractField transformation allows to extract a single field from a message and propagate that one. A related SMT is Debezium\u0026#8217;s SMT for change event flattening. It can be used to convert the complex Debezium change event structure with old and new row state, metadata and more, into a flat row representation, which can be consumed by many existing sink connectors.\n SMTs also allow to fine-tune schema namespaces; that can be of interest when working with a schema registry for managing schemas and their versions, and specific schema namespaces should be enforced for the messages on given topics. Two more, very useful examples of SMTs in this category are kafka-connect-transform-xml and kafka-connect-json-schema by Jeremy Custenborder, which will take XML or text and produce a typed Kafka Connect Struct, based on a given XML schema or JSON schema, respectively.\n Lastly, as a special kind of format conversion, SMTs can be used to modify or set the key of Kafka records. This may be desirable if a source connector doesn\u0026#8217;t produce any meaningful key, but one can be extracted from the record value. Also changing the message key can be useful, when considering subsequent stream processing. Choosing matching keys right at the source side e.g. allows for joining multiple topics via Kafka Streams, without the need for re-keying records.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs are the perfect tool for format conversions of Kafka Connect records\n   Ensuring Backwards Compatibility Changes to the schema of Kafka records can potentially be disruptive for consumers. If for instance a record field gets renamed, a consumer must be adapted accordingly, reading the value using the new field name. In case a field gets dropped altogether, consumers must not expect this field any longer.\n Message transformations can help with such transition from one schema version to the next, thus reducing the coupling of the lifecycles of message producers and consumers. In case of a renamed field, an SMT could add the field another time, using the original name. That\u0026#8217;ll allow consumers to continue reading the field using the old name and to be upgraded to use the new name at their own pace. After some time, once all consumers have been adjusted, the SMT can be removed again, only exposing the new field name going forward. Similarly, a field that got removed from a message schema could be re-added, e.g. using some sort of constant placeholder value. In other cases it might be possible to derive the field value from other, still existing fields. Again consumers could then be updated at their own pace to not expect and access that field any longer.\n It should be said though that there are limits for this usage: e.g. when changing the type of a field, things quickly become tricky. One option could be a multi-step approach where at first a separate field with the new type is added, before renaming it again as described above.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎\u0026nbsp;\u0026nbsp; SMTs can primarily help to address basic compatibility concerns around schema evolution\n   Filtering and Routing When applied on the source side, SMTs allow to filter out specific records produced by the connector. They also can be used for controlling the Kafka topic a record gets sent to. That\u0026#8217;s in particular interesting when filtering and routing is based on the actual record contents. In an IoT scenario for instance where Kafka Connect is used to ingest data from some kind of sensors, an SMT might be used to filter out all sensor measurements below a certain threshold, or route measurement events above a threshold to a special topic.\n Debezium provides a range of SMTs for record filtering and routing:\n   The logical topic routing SMT allows to send change events originating from multiple tables to the same Kafka topic, which can be useful when working with partition tables in Postgres, or with data that is sharded into multiple tables\n  The Filter and ContentBasedRouter SMTs let you use script expressions in languages such as Groovy or JavaScript for filtering and routing change events based on their contents; such script-based approach can be an interesting middleground between ease-of-use (no Java code must be compiled and deployed to Kafka Connect) and expressiveness; e.g. here is how the routing SMT could be used with GraalVM\u0026#8217;s JavaScript engine for routing change events from a table with purchase orders to different topics in Kafka, based on the order type:\n... transforms=route transforms.route.type=io.debezium.transforms.ContentBasedRouter transforms.route.topic.regex=.*purchaseorders transforms.route.language=jsr223.graal.js transforms.route.topic.expression= value.after.ordertype == 'B2B' ? 'b2b_orders' : 'b2c_orders' ...     The outbox event router comes in handy when implementing the transactional outbox pattern for data propagation between microservices: it can be used to send events originating from a single outbox table to a specific Kafka topic per aggregate (when thinking of domain driven design) or event type\n   There are also two SMTs for routing purposes in Kafka Connect itself: RegexRouter which allows to re-route records two different topics based on regular expressions, and TimestampRouter for determining topic names based on the record\u0026#8217;s timestamp.\n While routing SMTs usually are applied to source connectors (defining the Kafka topic a record gets sent to), it can also make sense to use them with sink connectors. That\u0026#8217;s the case when a sink connector derives the name of downstream table names, index names or similar from the topic name.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎📎\u0026nbsp;\u0026nbsp; Message filtering and topic routing\u0026#8201;\u0026#8212;\u0026#8201;no problem for SMTs\n   Tombstone Handling Tombstone records are Kafka records with a null value. They carry special semantics when working with compacted topics: during log compaction, all records with the same key as a tombstone record will be removed from the topic.\n Tombstones will be retained on a topic for a configurable time before compaction happens (controlled via delete.retention.ms topic setting), which means that also Kafka Connect sink connectors need to handle them. Unfortunately though, not all connectors are prepared for records with a null value, typically resulting in NullPointerExceptions and similar. A filtering SMT such as the one above can be used to drop tombstone records in such case.\n But also the exact opposite\u0026#8201;\u0026#8212;\u0026#8201;producing tombstone records\u0026#8201;\u0026#8212;\u0026#8201;can be useful: some sink connectors use tombstone records as the indicator to delete corresponding rows from a downstream datastore. Now when using a CDC connector like Debezium to capture changes from a database where \"soft deletes\" are used (i.e. records are not physically deleted, but a logically deleted flag is set to true when deleting a record), those change events will be exported as update events (which they technically are). A bespoke SMT can be used to translate these update events into tombstone records, triggering the deletion of corresponding records in downstream datastores.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs work well to discard tombstones or convert soft delete events into tombstones. What\u0026#8217;s not possible though is to keep the original event and produce an additional tombstone record at the same time\n   Externalizing Large Payloads Even some advanced enterprise application patterns can be implemented with the help of SMTs, one example being the claim check pattern. This pattern comes in handy in situations like this:\n  A message may contain a set of data items that may be needed later in the message flow, but that are not necessary for all intermediate processing steps. We may not want to carry all this information through each processing step because it may cause performance degradation and makes debugging harder because we carry so much extra data.\n \u0026#8201;\u0026#8212;\u0026#8201;Gregor Hohpe, Bobby Woolf; Enterprise Application Patterns\n   A specific example could again be a CDC connector that captures changes from a database table Users, with a BLOB column that contains the user\u0026#8217;s profile picture (surely not a best practice, still not that uncommon in reality\u0026#8230;\u0026#8203;).\n     Apache Kafka and Large Messages Apache Kafka isn\u0026#8217;t meant for large messages. The maximum message size is 1 MB by default, and while this can be increased, benchmarks are showing best throughput for much smaller messages. Strategies like chunking and externalizing large payloads can thus be vital in order to ensure a satisfying performance.\n     When propagating change data events from that table to Apache Kafka, adding the picture data to each event poses a significant overhead. In particular, if the picture BLOB hasn\u0026#8217;t changed between two events at all.\n Using an SMT, the BLOB data could be externalized to some other storage. On the source side, the SMT could extract the image data from the original record and e.g. write it to a network file system or an Amazon S3 bucket. The corresponding field in the record would be updated so it just contains the unique address of the externalised payload, such as the S3 bucket name and file path:\n   As an optimization, it could be avoided to re-upload unchanged file contents another time by comparing earlier and current hash of the externalized file.\n A corresponding SMT instance applied to sink connectors would retrieve the identifier of the externalized files from the incoming record, obtain the contents from the external storage and put it back into the record before passing it on to the connector.\n Mac\u0026#8217;s rating: \u0026nbsp;\u0026nbsp; 📎📎📎📎\u0026nbsp;\u0026nbsp; SMTs can help to externalize payloads, avoiding large Kafka records. Relying on another service increases overall complexity, though\n   Limitations As we\u0026#8217;ve seen, single message transformations can help to address quite a few requirements that commonly come up for users of Kafka Connect. But there are limitations, too; Like MacGyver, who sometimes has to reach for some other tool than his beloved Swiss Army knife, you shouldn\u0026#8217;t think of SMTs as the perfect solution all the time.\n The biggest shortcoming is already hinted at in their name: SMTs only can be used to process single records, one at a time. E.g. you cannot split up a record into multiple ones using an SMT, as they only can return (at most) one record. Also any kind of stateful processing, like aggregating data from multiple records, or correlating records from several topics is off limits for SMTs. For such use cases, you should be looking at stream processing technologies like Kafka Streams and Apache Flink; also integration technologies like Apache Camel can be of great use here.\n One thing to be aware of when working with SMTs is configuration complexity; when using generic, highly configurable SMTs, you might end up with lengthy configuration that\u0026#8217;s hard to grasp and debug. You might be better off implementing a bespoke SMT which is focussing on one particular task, leveraging the full capabilities of the Java programming language.\n     SMT Testing Whether you use ready-made SMTs by means of configuration, or you implement custom SMTs in Java, testing your work is essential.\n While unit tests are a viable option for basic testing of bespoke SMT implementations, integration tests running against Kafka Connect connectors are recommended for testing SMT configurations. That way you\u0026#8217;ll be sure that the SMT can process actual messages and it has been configured the way you intended to.\n Testcontainers and the Debezium support for Testcontainers are a great foundation for setting up all the required components such as Apache Kafka, Kafka Connect, connectors and the SMTs to test.\n     A specific feature I wished for every now and then is the ability to apply SMTs only to a specific sub-set of the topics created or consumed by a connector. In particular if connectors create different kinds of topics (like an actual data topic and another one with with metadata), it can be desirable to apply SMTs only to the topics of one group but not the other. This requirement is captured in KIP-585 (\"Filter and Conditional SMTs\"), please join the discussion on that one if you got requirements or feedback related to that.\n   Learning More There are several great presentations and blog posts out there which describe in depth what SMTs are, how you can implement your own one, how they are configured etc.\n Here are a few resources I found particularly helpful:\n   KIP-66: The original KIP (Kafka Improvement Proposal) that introduced SMTs\n  Singe Message Transforms are not the Transformations You\u0026#8217;re Looking For: A great overview on SMTs, their capabilities as well as limitations, by Ewen Cheslack-Postava\n  A hands-on experience with Kafka Connect SMTs: In-depth blog post on SMT use cases, things to be aware of and more, by Gian D\u0026#8217;Uia\n   Now, considering this wide range of use cases for SMTs, would MacGyver like and use them for implementing various tasks around Kafka Connect? I would certainly think so. But as always, the right tool for the job must be chosen: sometimes an SMT may be a great fit, another time a more flexible (and complex) stream processing solution might be preferable.\n Just as MacGyver, you got to make a call when to use your Swiss Army knife, duct tape or a paper clip.\n Many thanks to Hans-Peter Grahsl for his feedback while writing this blog post!\n  ","id":3,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eDo you remember Angus \"Mac\" MacGyver?\nThe always creative protagonist of the popular 80ies/90ies TV show, who could solve about any problem with nothing more than a Swiss Army knife, duct tape, shoe strings and a paper clip?\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe single message transformations (SMTs) of Kafka Connect are almost as versatile as MacGyver\u0026#8217;s Swiss Army knife:\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"ulist\"\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the timezone or format of date/time message fields?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to change the topic a specific message gets sent to?\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eHow to filter out specific records?\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eSMTs can be the answer to these and many other questions that come up in the context of Kafka Connect.\nApplied to source or sink connectors,\nSMTs allow to modify Kafka records before they are sent to Kafka, or after they are consumed from a topic, respectively.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Single Message Transformations - The Swiss Army Knife of Kafka Connect","uri":"http://localhost:1313/blog/single-message-transforms-swiss-army-knife-of-kafka-connect/","year":"2020"},{"content":"For libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via service provider interfaces (SPIs): contracts to be implemented by the application developer, which then are invoked by framework code, adding new or replacing existing functionality.\n Often times, the method implementations of such an SPI need to return value(s) to the framework. An alternative to return values are \"emitter parameters\": passed by the framework to the SPI method, they offer an API for receiving value(s) via method calls. Certainly not revolutionary or even a new idea, I find myself using emitter parameters more and more in libraries and frameworks I work on. Hence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\n An Example As an example, let\u0026#8217;s consider a blogging platform which provides an SPI for extracting categories and tags from given blog posts. Application developers can plug in custom implementations of that SPI, e.g. based on the latest and greatest algorithms in information retrieval and machine learning. Here\u0026#8217;s how a basic SPI contract for this use case could look like, using regular method return values:\n 1 2 3 4 5 public interface BlogPostDataExtractor { Set\u0026lt;String\u0026gt; extractCategories(String contents); Set\u0026lt;String\u0026gt; extractTags(String contents); }    This probably would get the job done, but there are a few problems: any implementation will have to do two passes on the given blog post contents, once in each method\u0026#8201;\u0026#8212;\u0026#8201;not ideal. Also let\u0026#8217;s assume that most blog posts only belong to exactly one category. Implementations still would have to allocate a set for the single returned category.\n While there\u0026#8217;s not much we can do about the second issue with a return value based design, the former problem could be addressed by combining the two methods:\n 1 2 3 4 public interface BlogPostDataExtractor { CategoriesAndTags extractCategoriesAndTags(String contents); }    Now an implementation can retrieve both categories and tags at once. But it\u0026#8217;s worth thinking about how an SPI implementation would instantiate the return type.\n Exposing a concrete class to be instantiated by implementors poses a challenge for future evolution of the SPI: following the best practice and making the return object type immutable, all its properties must be passed to its constructor. Now if an additional attribute should be extracted from blog posts, such as a teaser, the existing constructor cannot be modified, so to not break existing user code. Instead, we\u0026#8217;d have to introduce new constructors whenever adding further attributes. Dealing with all these constructors could become quite inconvenient, in particular if a specific SPI implementation is only interested in producing some of the attributes.\n All in all, for SPIs it\u0026#8217;s often a good idea to only expose interfaces, but no concrete classes. So we could make the return type an interface and leave it to SPI implementors to create an implementation class, but that\u0026#8217;d be rather tedious.\n   The Emitter Parameter Pattern Or, we could provide some sort of builder object which can be used to construct CategoriesAndTags objects. But then why even return an object at all, instead of simply mutating the state of a builder that is provided through a method parameter? And that\u0026#8217;s essentially what the emitter parameter pattern is about: passing in an object which can be used to emit the values which should be \"returned\" by the method.\n     I\u0026#8217;m not aware of any specific name for this pattern, so I came up with \"emitter parameter pattern\" (the notion of callback parameters is related, yet different). And hey, perhaps I\u0026#8217;ll become famous for coining a design pattern name ;) Please let me know in the comments below if you know this pattern under a different name.\n     Here\u0026#8217;s how the extractor SPI could look like when designed with an emitter parameter:\n 1 2 3 4 5 6 7 8 9 10 public interface BlogPostDataExtractor { void extractData(String contents, BlogPostDataReceiver data); (1) interface BlogPostDataReceiver { (2) void addCategory(String category); void addTag(String tag); } }      1 SPI method with input parameter and emitter parameter   2 Emitter parameter type    An implementation would emit the retrieved information by invoking the methods on the data parameter:\n 1 2 3 4 5 6 7 8 9 10 public class MyBlogPostDataExtractor implements BlogPostDataExtractor { public void extractData(String contents, BlogPostDataReceiver data) { String category = ...; Stream\u0026lt;String\u0026gt; tags = ...; data.addCatgory(category); tags.forEach(data::addTag); } }    This approach nicely avoids all the issues with the return value based design:\n   Single and multiple value case handled uniformly: an implementation can call addCategory() just once, or multiple times; either way, it doesn\u0026#8217;t have to deal with the creation of a set, list, or other container for the produced value(s)\n  Flexible evolution of the SPI contract: new methods such as addTeaser(), or addTags(String\u0026#8230;\u0026#8203; tags) can be added to the emitter parameter type, avoiding the creation of more and more return type constructors; as the passed BlogPostDataReceiver instance is controlled by the framework itself, we also could add methods which provide more context required for the task at hand\n  No need for exposing concrete types on the SPI surface: as no return value needs to be instantiated by SPI implementations, the solution works solely with interfaces on the SPI surface; this provides more control to the framework, e.g. the emitter object could be re-used etc.\n  Flexible implementation choices: by not requiring SPI implementations to allocate any return objects, the platform gains a lot of flexibility for how it\u0026#8217;s processing the emitted values: while it could collect the values in a set or list, it also has the option to not allocate any intermediary collections, but process and pass on values one-by-one in a streaming-based way, without any of this impacting SPI implementors\n   Now, are there some downsides to this approach, too? I can see two: if a method only ever should yield a single value, the emitter API might be misleading. We could raise an exception though if an emitter method is called more than once. Also an implementation might hold on to the emitter object and invoke its methods after the call flow has returned from the SPI method, which typically isn\u0026#8217;t desirable. Again that\u0026#8217;s something that can be prevented by invalidating the emitter object after the SPI method returned, raising an exception in case of further method invocations.\n Overall, I think the emitter parameter pattern is a valuable tool in the box of library and framework authors; it provides flexibility for implementation choices and future evolution when designing SPIs. Real-world examples include the ValueExtractor SPI in Bean Validation 2.0 (where it was chosen to provide a uniform value of extracting single and multiple values from container objects) and the ChangeRecordEmitter contract in Debezium\u0026#8217;s SPI.\n Many thanks to Hans-Peter Grahsl and Nils Hartmann for reviewing an early version of this blog post.\n  ","id":4,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eFor libraries and frameworks it\u0026#8217;s a common requirement to make specific aspects customizeable via \u003ca href=\"https://en.wikipedia.org/wiki/Service_provider_interface\"\u003eservice provider interfaces\u003c/a\u003e (SPIs):\ncontracts to be implemented by the application developer, which then are invoked by framework code,\nadding new or replacing existing functionality.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOften times, the method implementations of such an SPI need to return value(s) to the framework.\nAn alternative to return values are \"emitter parameters\":\npassed by the framework to the SPI method, they offer an \u003cem\u003eAPI\u003c/em\u003e for receiving value(s) via method calls.\nCertainly not revolutionary or even a new idea,\nI find myself using emitter parameters more and more in libraries and frameworks I work on.\nHence I\u0026#8217;d like to discuss some advantages I perceive about the emitter parameter pattern.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"The Emitter Parameter Pattern for Flexible SPI Contracts","uri":"http://localhost:1313/blog/emitter-parameter-pattern-for-flexible-spis/","year":"2020"},{"content":"Making applications extensible with some form of plug-ins is a very common pattern in software design: based on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality. Examples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect, a runtime for Kafka connectors plug-ins.\n In this post I\u0026#8217;m going to explore how the Java Platform Module System's notion of module layers can be leveraged for implementing plug-in architectures on the JVM. We\u0026#8217;ll also discuss how Layrry, a launcher and runtime for layered Java applications, can help with this task.\n A key requirement for any plug-in architecture is strong isolation between different plug-ins: their state, classes and dependencies should be encapsulated and independent of each other. E.g. package declarations in two plug-ins should not collide, also they should be able to use different versions of another 3rd party dependency. This is why the default module path of Java (specified using the --module-path option) is not enough for this purpose: it doesn\u0026#8217;t support more than one version of a given module.\n The module system\u0026#8217;s answer are module layers: by organizing an application and its plug-ins into multiple layers, the required isolation between plug-ins can be achieved.\n     With the module system, each Java application always contains at least one layer, the boot layer. It contains the platform modules and the modules provided on the module path.\n     An Example: The Greeter CLI App To make things more tangible, let\u0026#8217;s consider a specific example; The \"Greeter\" app is a little CLI utility, that can produce greetings in different languages.\n In order to not limit the number of supported languages, it provides a plug-in API, which allows to add additional greeting implementations, without the need to rebuild the core application. Here is the Greeter contract, which is to be implemented by each language plug-in:\n 1 2 3 4 5 package com.example.greeter.api; public interface Greeter { String greet(String name); }    Greeters are instantiated via accompanying implementations of GreeterFactory:\n 1 2 3 4 5 public interface GreeterFactory { String getLanguage(); (1) String getFlag(); Greeter getGreeter(); (2) }      1 The getLanguage() and getFlag() methods are used to show a description of all available greeters in the CLI application   2 The getGreeter() method returns a new instance of the corresponding Greeter type    Here\u0026#8217;s the overall architecture of the Greeter application, with three different language implementations:\n   The application is made up of five different layers:\n   greeter-platform: contains the Greeter and GreeterFactory contracts\n  greeter-en, greeter-de and greeter-fr: greeter implementations for different languages; note how each one is depending on a different version of some greeter-date module. As they are isolated in different layers, they can co-exist within the application\n  greeter-app: the \"shell\" of the application which loads all the greeter implementations and makes them accessible as a simple CLI application\n   Now let\u0026#8217;s see how this application structure can be assembled using Layrry.\n   Application Plug-ins With Layrry In a previous blog post we\u0026#8217;ve explored how applications can be cut into layers, described in Layrry\u0026#8217;s layers.yml configuration file. A simple static layer definition would defeat the purpose of a plug-in architecture, though: not all possible plug-ins are known when assembling the application.\n Layrry addresses this requirement by allowing to source different layers from directories on the file system:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 layers: platform: (1) modules: - \"com.example.greeter:greeter-api:1.0.0\" plugins: (2) parents: - \"api\" directory: path/to/plugins app: (3) parents: - \"plugins\" modules: - \"com.example.greeter:greeter-app:1.0.0\" main: module: com.example.greeter.app class: com.example.greeter.app.App      1 The platform layer with the API module   2 The plug-in layer(s)   3 The application layer with the \"application shell\"    Whereas the platform and app layers are statically defined, using the Maven GAV coordinates of the modules to include, the plugins part of the configuration describes an open-ended set of layers. Each sub-directory of the given directory represents its own layer. All modules within this sub-directory will be added to the layer, and the API layer will be the parent of each of the plug-in layers. The app layer has all the plug-in layers as its ancestors, allowing it to retrieve plug-in implementations from these layers.\n More greeter plug-ins can be added to the application by simply creating a sub-directory with the required module(s).\n   Finding Plug-in Implementations With the Java Service Loader Structuring the application into different layers isn\u0026#8217;t all we need for building a plug-in architecture; we also need a way for detecting and loading the actual plug-in implementations. The service loader mechanism of the Java platform comes in handy for that. If you have never worked with the service loader API, it\u0026#8217;s definitely recommended to study its extensive JavaDoc description:\n  A service is a well-known interface or class for which zero, one, or many service providers exist. A service provider (or just provider) is a class that implements or subclasses the well-known interface or class. A ServiceLoader is an object that locates and loads service providers deployed in the run time environment at a time of an application\u0026#8217;s choosing.   Having been a supported feature of Java since version 6, the service loader API has been been reworked and refined to work within modular environments when the Java Module System was introduced in JDK 9.\n In order to retrieve service implementations via the service loader, a consuming module must declare the use of the service in its module descriptor. For our purposes, the GreeterFactory contract is a perfect examplification of the service idea. Here\u0026#8217;s the descriptor of the Greeter application\u0026#8217;s app module, declaring its usage of this service:\n 1 2 3 4 5 module com.example.greeter.app { exports com.example.greeter.app; requires com.example.greeter.api; uses com.example.greeter.api.GreeterFactory; }    The module descriptor of each greeter plug-in must declare the service implementation(s) which it provides. E.g. here is the module descriptor of the English greeter implementation:\n 1 2 3 4 5 6 module com.example.greeter.en { requires com.example.greeter.api; requires com.example.greeter.dateutil; provides com.example.greeter.api.GreeterFactory with com.example.greeter.en.EnglishGreeterFactory; }    From within the app module, the service implementations can be retrieved via the java.util.ServiceLoader class.\n When using the service loader in layered applications, there\u0026#8217;s one potential pitfall though, which mostly will affect existing applications which are migrated: in order to access service implementations located in a different layer (specifically, in an ancestor layer of the loading layer), the method load(ModuleLayer, Class\u0026lt;?\u0026gt;) must be used. When using other overloaded variants of load(), e.g. the commonly used load(Class\u0026lt;?\u0026gt;), those implementations won\u0026#8217;t be found.\n Hence the code for loading the greeter implementations from within the app layer could look like this:\n 1 2 3 4 5 6 7 8 9 10 private static List\u0026lt;GreeterFactory\u0026gt; getGreeterFactories() { ModuleLayer appLayer = App.class.getModule().getLayer(); return ServiceLoader.load(appLayer, GreeterFactory.class) .stream() .map(p -\u0026gt; p.get()) .sorted((gf1, gf2) -\u0026gt; gf1.getLanguage().compareTo( gf2.getLanguage())) .collect(Collectors.toList()); }    Having loaded the list of greeter factories, it doesn\u0026#8217;t take too much code to display a list with all available implementations, expect a choice by the user and invoke the greeter for the chosen language. This code which isn\u0026#8217;t too interesting is omitted here for the sake of brevity and can be found in the accompanying example source code repo.\n     JDK 9 brought some more nice improvements for the service loader API. E.g. the type of service implementations can be examined without actually instantiating them. This allows for interesting alternatives for providing service meta-data and choosing an implementation based on some criteria. For instance, greeter metadata like the language name and flag could be given using an annotation:\n 1 2 3 4 @GreeterDefinition(lang=\"English\", flag=\"🇬🇧\") public class EnglishGreeterFactory implements GreeterFactory { Greeter getGreeter(); }    Then the method ServiceLoader.Provider#type() can be used to obtain the annotation and return a greeter factory for a given language:\n 1 2 3 4 5 6 7 8 9 10 11 private Optional\u0026lt;GreeterFactory\u0026gt; getGreeterFactoryForLanguage( String language) { ModuleLayer layer = App.class.getModule().getLayer(); return ServiceLoader.load(layer, GreeterFactory.class) .stream() .filter(gf -\u0026gt; gf.type().getAnnotation( GreeterDefinition.class).lang().equals(language)) .map(gf -\u0026gt; gf.get()) .findFirst(); }          Seeing it in Action Lastly, let\u0026#8217;s take a look at the complete Greeter application in action. Here it is, initially with two, and then with three greeter implementations:\n   The layers configuration file is adjusted to load greeter plug-ins from the plugins directory; initially, two greeters for English and French exist. Then the German greeter implementation gets picked up by the application after adding it to the plug-in directory, without requiring any changes to the application tiself.\n The complete source code, including the logic for displaying all the available greeters and prompting for input, is available in the Layrry repository on GitHub.\n And there you have it, a basic plug-in architecture using Layrry and the Java Module System. Going forward, this might evolve in a few ways. E.g. it might be desirable to detect additional plug-ins without having to restart the application, e.g. when thinking of desktop application use cases. While loading additional plug-ins in new layers should be comparatively easy, unloading already loaded layers, e.g. when updating a plug-in to a newer version, could potentially be quite tricky. In particular, there\u0026#8217;s no way to actively unload layers, so we\u0026#8217;d have to rely on the garbage collector to clean up unused layers, making sure no references to any of their classes are kept in other, active layers.\n One also could think of an event bus, allowing different plug-ins to communicate in a safe, yet loosely coupled way. What requirements would you have for plug-in centered applications running on the Java Module System? Let\u0026#8217;s exchange in the comments below!\n  ","id":5,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eMaking applications extensible with some form of plug-ins is a very common pattern in software design:\nbased on well-defined APIs provided by the application core, plug-ins can customize an application\u0026#8217;s behavior and provide new functionality.\nExamples include desktop applications like IDEs or web browsers, build tools such as Apache Maven or Gradle, as well as server-side applications such as Apache Kafka Connect,\na runtime for Kafka connectors plug-ins.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to explore how the \u003ca href=\"https://www.jcp.org/en/jsr/detail?id=376\"\u003eJava Platform Module System\u003c/a\u003e's notion of module layers can be leveraged for implementing plug-in architectures on the JVM.\nWe\u0026#8217;ll also discuss how \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e, a launcher and runtime for layered Java applications, can help with this task.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Plug-in Architectures With Layrry and the Java Module System","uri":"http://localhost:1313/blog/plugin-architectures-with-layrry-and-the-java-module-system/","year":"2020"},{"content":"One of the biggest changes in recent Java versions has been the introduction of the module system in Java 9. It allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\n In this post I\u0026#8217;m going to introduce the Layrry open-source project, a launcher and Java API for executing modularized Java applications. Layrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers. Layers go beyond the capabilities of the \"flat\" module path specified via the --module-path parameter of the java command, e.g. allowing to use multiple versions of one module within one and the same application.\n Why Layrry? The Java Module System doesn\u0026#8217;t define any means of mapping between modules (e.g. com.acme.crm) and JARs providing such module (e.g. acme-crm-1.0.0.Final.jar), or retrieving modules from remote repositories using unique identifiers (e.g. com.acme:acme-crm:1.0.0.Final). Instead, it\u0026#8217;s the responsibility of the user to obtain all required JARs of a modularized application and provide them via the --module-path parameter.\n Furthermore, the module system doesn\u0026#8217;t define any means of module versioning; i.e. it\u0026#8217;s the responsibility of the user to obtain all modules in the right version. Using the --module-path option, it\u0026#8217;s not possible, though, to assemble an application that uses multiple versions of one and the same module. This may be desirable for transitive dependencies of an application, which might be required in different versions by two separate direct dependencies.\n This is where Layrry comes in (pronounced \"Larry\"): it provides a declarative approach as well as an API for assembling modularized applications. The (modular) JARs to be included are described using Maven GAV (group id, artifact id, version) coordinates, solving the issue of retrieving all required JARs from a remote repository, in the right version.\n With Layrry, applications are organized in module layers, which allows to use different versions of one and the same module in different layers of an application (as long as they are not exposed in a conflicting way on module API boundaries).\n   An Example As an example, let\u0026#8217;s consider an application made up of the following modules:\n   The application\u0026#8217;s main module, com.example:app, depends on two others, com.example:foo and com.example:bar. They in turn depend on the Log4j API and another module, com.example:greeter. The latter is used in two different versions, though.\n Let\u0026#8217;s take a closer look at the Greeter class in these modules. Here is the version in com.example:greeter@1.0.0, as used by com.example:foo:\n 1 2 3 4 5 6 public class Greeter { public String greet(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 1.0.0)\"; } }    And this is how it looks in com.example:greeter@2.0.0, as used by com.example:bar:\n 1 2 3 4 5 6 7 8 9 10 11 public class Greeter { public String hello(String name, String from) { return \"Hello, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } public String goodBye(String name, String from) { return \"Good bye, \" + name + \" from \" + from + \" (Greeter 2.0.0)\"; } }    The Greeter API has evolved in a backwards-incompatible way, i.e. it\u0026#8217;s not possible for the foo and bar modules to use the same version.\n With a \"flat\" module path (or classpath), there\u0026#8217;s no way for dealing with this situation. You\u0026#8217;d inevitably end up with a NoSuchMethodError, as either foo or bar would be linked at runtime against a version of the class different from the version it has been compiled against.\n The lack of support for using multiple module versions when working with the --module-path option might be surprising at first, but it\u0026#8217;s an explicit non-requirement of the module system to support multiple module versions or even deal with selecting matching module versions at all.\n This means that the module descriptors of both foo and bar require the greeter module without any version information:\n 1 2 3 4 5 module com.example.foo { exports com.example.foo; requires org.apache.logging.log4j; requires com.example.greeter; }    1 2 3 4 5 module com.example.bar { exports com.example.bar; requires org.apache.logging.log4j; requires com.example.greeter; }      Module Layers to the Rescue While only one version of a given module is supported when running applications via java --module-path=\u0026#8230;\u0026#8203;, there\u0026#8217;s a lesser known feature of the module system which provides a way out: module layers.\n A module layer \"is created from a graph of modules in a Configuration and a function that maps each module to a ClassLoader.\" Using the module layer API, multiple versions of a module can be loaded in different layers, thus using different classloaders.\n Note the layers API doesn\u0026#8217;t concern itself with obtaining JARs or modules from remote locations such as the Maven Central repository; instead, any modules must be provided as Path objects. Here is how a layer with the foo and greeter:1.0.0 modules could be assembled:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ModuleLayer boot = ModuleLayer.boot(); ClassLoader scl = ClassLoader.getSystemClassLoader(); Path foo = Paths.get(\"path/to/foo-1.0.0.jar\"); (1) Path greeter10 = Paths.get(\"path/to/greeter-1.0.0.jar\"); (2) ModuleFinder fooFinder = ModuleFinder.of(foo, greeter10); Configuration fooConfig = boot.configuration() (3) .resolve( fooFinder, ModuleFinder.of(), Set.of(\"com.example.foo\", \"com.example.greeter\") ); ModuleLayer fooLayer = boot.defineModulesWithOneLoader( fooConfig, scl); (4)      1 obtain foo-1.0.0.jar   2 obtain greeter-1.0.0.jar   3 Create a configuration derived from the \"boot\" module of the JVM, providing a ModuleFinder for the two JARs obtained before, and resolving the two modules   4 Create a module layer using the configuration, loading all contained modules with a single classloader    Similarly, you could create a layer for bar and greeter:2.0.0, as well as layers for log4j and the main application module. The layers API is very flexible, e.g. you could load each module in its own classloader and more. But all this flexibility can make using the API direcly a daunting task.\n Also using an API might not be what you want in the first place: wouldn\u0026#8217;t it be nice if there was a CLI tool, akin to using java --module-path=\u0026#8230;\u0026#8203;, but with the additional powers of module layers?\n   The Layrry Launcher This is where Layrry comes in: it is a CLI tool which takes a configuration of a layered application (defined in a YAML file) and executes it. The layer descriptor for the example above looks like so:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 layers: log: (1) modules: (2) - \"org.apache.logging.log4j:log4j-api:jar:2.13.1\" - \"org.apache.logging.log4j:log4j-core:jar:2.13.1\" - \"com.example:logconfig:1.0.0\" foo: parents: (3) - \"log\" modules: - \"com.example:greeter:1.0.0\" - \"com.example:foo:1.0.0\" bar: parents: - \"log\" modules: - \"com.example:greeter:2.0.0\" - \"com.example:bar:1.0.0\" app: parents: - \"foo\" - \"bar\" modules: - \"com.example:app:1.0.0\" main: (4) module: com.example.app class: com.example.app.App      1 Each layer has a unique name   2 The modules element lists all the modules contained in the layer, using Maven coordinates (group id, artifact id, version), unambigously referencing a (modular) JAR in a specific version   3 A layer can have one or more parent layers, whose modules it can access; if no parent is given, the JVM\u0026#8217;s \"boot\" layer is the implicit parent of a layer   4 The given main module and class is the one that will be executed by Layrry    The configuration above describes four layers, log, foo, bar and app, with the modules they contain and the parent/child relationships between these layers. Note how the versions 1.0.0 and 2.0.0 of the greeter module are used in foo and bar. The file also specifies the main class to execute when running this application.\n Using Layrry, a modular application is executed like this:\n 1 2 3 4 5 6 7 java -jar layrry-1.0-SNAPSHOT-jar-with-dependencies.jar \\ --layers-config layers.yml \\ Alice 20:58:01.451 [main] INFO com.example.foo.Foo - Hello, Alice from Foo (Greeter 1.0.0) 20:58:01.472 [main] INFO com.example.bar.Bar - Hello, Alice from Bar (Greeter 2.0.0) 20:58:01.473 [main] INFO com.example.bar.Bar - Good bye, Alice from Bar (Greeter 2.0.0)    The log messages show how the two versions of greeter are used by foo and bar, respectively. Layrry will download all referenced JARs using the Maven resolver API, i.e. you don\u0026#8217;t have to deal with manually obtaining all the JARs and providing them to the java runtime.\n   Using the Layrry API In addition to the YAML-based launcher, Layrry provides also a Java API for assembling and running layered applications. This can be used in cases where the structure of layers is only known at runtime, or for implementing plug-in architectures.\n In order to use Layrry programmatically, add the following dependency to your pom.xml:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.moditect.layrry\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;layrry\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    Then, the Layrry Java API can be used like this (showing the same example as above):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 Layers layers = Layers.layer(\"log\") .withModule(\"org.apache.logging.log4j:log4j-api:jar:2.13.1\") .withModule(\"org.apache.logging.log4j:log4j-core:jar:2.13.1\") .withModule(\"com.example:logconfig:1.0.0\") .layer(\"foo\") .withParent(\"log\") .withModule(\"com.example:greeter:1.0.0\") .withModule(\"com.example:foo:1.0.0\") .layer(\"bar\") .withParent(\"log\") .withModule(\"com.example:greeter:2.0.0\") .withModule(\"com.example:bar:1.0.0\") .layer(\"app\") .withParent(\"foo\") .withParent(\"bar\") .withModule(\"com.example:app:1.0.0\") .build(); layers.run(\"com.example.app/com.example.app.App\", \"Alice\");      Next Steps The Layrry project is still in its infancy. Nevertheless it can be a useful tool for application developers wishing to leverage the Java Module System. Obtaining modular JARs via Maven coordinates and providing an easy-to-use mechanism for organizing modules in layers enables usages which cannot be addressed using the plain java --module-path \u0026#8230;\u0026#8203; approach.\n Layrry is open-source (under the Apache License version 2.0). The source code is hosted on GitHub, and your contributions are very welcomed.\n Please let me know about your ideas and requirements in the comments below or by opening up issues on GitHub. Planned enhancements include support for creating modular runtime images (jlink) based on the modules referenced in a layers.yml file, and visualization of module layers and their modules via GraphViz.\n  ","id":6,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the biggest changes in recent Java versions has been the introduction of the \u003ca href=\"http://openjdk.java.net/projects/jigsaw/spec/\"\u003emodule system\u003c/a\u003e in Java 9.\nIt allows to organize Java applications and their dependencies in strongly encapsulated modules, utilizing explicit and well-defined module APIs and relationships.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this post I\u0026#8217;m going to introduce the \u003ca href=\"https://github.com/moditect/layrry\"\u003eLayrry\u003c/a\u003e open-source project, a launcher and Java API for executing modularized Java applications.\nLayrry helps Java developers to assemble modularized applications from dependencies using their Maven coordinates and execute them using module layers.\nLayers go beyond the capabilities of the \"flat\" module path specified via the \u003cem\u003e--module-path\u003c/em\u003e parameter of the \u003cem\u003ejava\u003c/em\u003e command,\ne.g. allowing to use multiple versions of one module within one and the same application.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Introducing Layrry: A Launcher and API for Modularized Java Applications","uri":"http://localhost:1313/blog/introducing-layrry-runner-and-api-for-modularized-java-applications/","year":"2020"},{"content":"Within Debezium, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict. As others where interested in how we addressed the issue, and also for our own future reference, I\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\n The Problem Ideally, we\u0026#8217;d only ever work on a single branch and would never have to deal with porting changes between the master and other branches. Oftentimes we cannot get around this, though: specific versions of a software may have to be maintained for some time, requiring to backport bugfixes from the current development branch to the branch corresponding to the maintained version.\n In our specific case we had to deal with backporting changes to our project documentation. To complicate things, this documentation (written in AsciiDoc) has been largely re-organized between master and the targeted older branch, 1.0. What used to be one large AsciiDoc file for each of the Debezium connectors, got split up into multiple smaller files on master now. This split was meant to be applied to 1.0 too, but due to some miscommunication in the team (these things happen, right) this wasn\u0026#8217;t done, whereas an asorted set of documentation changes had been backported already to the larger, monolithic AsciiDoc files.\n So the situation we faced was this:\n   large, monolithic AsciiDoc files on the 1.0 branch\n  smaller, modularized AsciiDoc files on master\n  Documentation updates applied on master, of which only a subset is relevant for 1.0 (new features shouldn\u0026#8217;t be added to the Debezium 1.0 documentation)\n  Some of the documentation updates relevant for the 1.0 branch already had been backported from master, while others had not\n   All in all, a rather convoluted situation; the full diff of the documentation sub-directory between the two branches was about 13K lines.\n So what should we do? Cherry-picking individual commits from master was not really an option, as there were a few hundred commits on master since 1.0 had been forked off. Also many commits would contain documentation and code changes. The latter had already been backported successfully before.\n Realizing that resolving that merge conflict was next to impossible, the next idea was to essentially start from scratch and re-apply all relevant documentation changes to the 1.0 branch. Our initial idea was to create a patch with the difference of the documentation directory between the two branches. But editing that patch file with 13K lines turned out to be not manageable, either.\n   The Solution This is when we were reminded of the possibilities of git filter-branch: using this command it should be possible to isolate all the documentation changes done on master since Debezium 1.0 and apply the required sub-set of these changes to the 1.0 branch.\n To start with a clean slate, we created a new temporary branch based on 1.0:\n git checkout -b docs_backport 1.0   We then reset the contents of the documentation directory to its state as of the 1.0.0.Final release, as that\u0026#8217;s where the 1.0 and master branches diverged.\n rm -rf documentation git add documentation git checkout v1.0.0.Final documentation git commit -m \"Resetting documentation dir to v1.0.0.Final\" # This should yield no differences git diff v1.0.0.Final..docs_backport documentation   The next step was to filter all commits on master so to only keep any changes to the documentation directory. This was done on a new branch, docs_filtered. The --subdirectory-filter option comes in handy for that:\n git checkout -b docs_filtered master git filter-branch -f --prune-empty \\ --subdirectory-filter documentation \\ v1.0.0.Final..docs_filtered   This leaves us with a branch docs_filtered which only contains the commits since the v1.0.0.Final tag that modified the documentation directory.\n The --subdirectory-filter option also moves the contents of the given directory to the root of the repo, though. That\u0026#8217;s not exactly what we need. But another option, --tree-filter, lets us restore the original directory layout. It allows to run a set of commands against each of the filtered commits. We can use this to move the contents of documentation back to that directory:\n git filter-branch -f \\ --tree-filter 'mkdir -p documentation; \\ mv antora.yml documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null; \\ mv modules documentation 1\u0026gt;/dev/null 2\u0026gt;/dev/null;' \\ v1.0.0.Final..docs_filtered   Examining the history now, we can see that the commits on the docs_filtered apply the changes to the documentation directory, as expected.\n One problem still remains, though: by means of the --subdirectory-filter option, the very first commit removes all contents besides the documentation directory. This can be fixed by doing an interactive rebase of the current branch, beginning at the v1.0.0.Final tag:\n git rebase -i v1.0.0.Final   We need to edit the very first commit; all changes besides those to the documentation directory need to be reverted from that commit. There might be a better way of doing so, I simply ran git checkout for all the other resources:\n git checkout v1.0.0.Final debezium-connector-mongodb git checkout v1.0.0.Final debezium-connector-mysql ...   At this point the filtered branch still is based off of the v1.0.0.Final tag, whereas it should be based off of the docs_backport branch. git rebase --onto to the rescue:\n git rebase --onto docs_backport v1.0.0.Final docs_filtered   This rebases all the commits from the docs_filtered branch onto the docs_backport branch. Now we have a state where where all the documention changes have been cleanly applied to the 1.0 code base, i.e. the following should yield no differences:\n git diff docs_filtered..master documentation   The last and missing step is to do another rebase of all the documentation commits, discarding those that apply to any features that didn\u0026#8217;t get backported to 1.0.\n Thankfully, my partner-in-crime Jiri Pechanec stepped in here: as he had done the original feature backport, it didn\u0026#8217;t take him too long to go through the list of documentation commits and identify those which were relevant for the 1.0 code base. After one more interactive rebase for applying those we finally were in a state, where all the required documentation changes had been backported.\n Looking at the 1.0 history, you\u0026#8217;d still see some partial documentation changes up to the point, where we decided to start all over and revert these. Theoretically we could do another git filter run to exclude those, but we decided against that, as we already had done releases off of the 1.0 branch and didn\u0026#8217;t want to alter the commit history of a released branch after the fact.\n  ","id":7,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWithin \u003ca href=\"https://debezium.io/\"\u003eDebezium\u003c/a\u003e, the project I\u0026#8217;m working on at Red Hat, we recently encountered an \"interesting\" situation where we had to resolve a rather difficult merge conflict.\nAs others where interested in how we addressed the issue, and also for our own future reference,\nI\u0026#8217;m going to give a quick run down of the problem we encountered and how we solved it.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Reworking Git Branches with git filter-branch","uri":"http://localhost:1313/blog/reworking-git-branches-with-git-filter-branch/","year":"2020"},{"content":"The JDK Flight Recorder (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications. Open-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\n In this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more. We\u0026#8217;ll also discuss how the JFR Event Streaming API new in Java 14 can be used to export live events, making them available for monitoring and alerting via tools such as Prometheus and Grafana.\n JFR and its companion tool JDK Mission Control (JMC) for analyzing JFR recordings have come a long way; originally developed at BEA and part of the JRockit VM, they were later on commercial features of the Oracle JDK. As of Java 11, JFR got open-sourced and is part of OpenJDK distributions. JMC is also open-source, but it\u0026#8217;s an independent tool under the OpenJDK umbrella, which must be downloaded separately.\n Using the combination of JFR and JMC, you can get all kinds of information about your Java application, such as events on garbage collection, compilation, classloading, memory allocation, file and socket IO, method profiling data, and much more. To learn more about Flight Recorder and Mission Control in general, have a look at the Code One 2019 presentation Introduction to JDK Mission Control \u0026amp; JDK Flight Recorder by Marcus Hirt and Klara Ward. You can find some more links to related useful resources towards the end of this post.\n Custom Flight Recorder Events One thing that\u0026#8217;s really great about JFR and JMC is that you\u0026#8217;re not limited to the events and data baked into the JVM and platform libraries: JFR also provides an API for implementing custom events. That way you can use the low-overhead event recording infrastructure (its goal is to add at most 1% performance overhead) for your own event types. This allows you to record and analyze higher-level events, using the language of your application-specific domain.\n Taking my day job project Debezium as an example (an open-source platform for change data capture for a variety of databases), we could for instance produce events such as \"Snapshot started\", \"Snapshotting of table 'Customers' completed\", \"Captured change event for transaction log offset 123\" etc. Users could send us recordings with these events and we could dive into them, in order to identify bugs or performance issues.\n In the following let\u0026#8217;s consider a less complex and hence better approachable example, though. We\u0026#8217;ll implement an event for measuring the duration of REST API calls. The Todo service from my recent blog post on Quarkus Qute will serve as our guinea pig. It is based on the Quarkus stack and provides a simple REST API based on JAX-RS. As always, you can find the complete source code for this blog post on GitHub.\n Event types are implemented by extending the jdk.jfr.Event class; It already provides us with some common attributes such as a timestamp and a duration. In sub-classes you can add application-specific payload attributes, as well as some metadata such as a name and category which will be used for organizing and displaying events when looking at them in JMC.\n Which attributes to add depends on your specific requirements; you should aim for the right balance between capturing all the relevant information that will be useful for analysis purposes later on, while not going overboard and adding too much, as that could cause record files to become too large, in particular for events that are emitted with a high frequency. Also retrieval of the attributes should be an efficient operation, so to avoid any unneccessary overhead.\n Here\u0026#8217;s a basic event class for monitoring our REST API calls:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 @Name(JaxRsInvocationEvent.NAME) (1) @Label(\"JAX-RS Invocation\") @Category(\"JAX-RS\") @Description(\"Invocation of a JAX-RS resource method\") @StackTrace(false) (2) public class JaxRsInvocationEvent extends Event { static final String NAME = \"dev.morling.jfr.JaxRsInvocation\"; @Label(\"Resource Method\") (3) public String method; @Label(\"Media Type\") public String mediaType; @Label(\"Java Method\") public String javaMethod; @Label(\"Path\") public String path; @Label(\"Query Parameters\") public String queryParameters; @Label(\"Headers\") public String headers; @Label(\"Length\") @DataAmount (4) public int length; @Label(\"Response Headers\") public String responseHeaders; @Label(\"Response Length\") public int responseLength; @Label(\"Response Status\") public int status; }      1 The @Name, @Category, @Description and @Label annotations define some meta-data, e.g. used for controlling the appearance of these events in the JMC UI   2 JAX-RS invocation events shouldn\u0026#8217;t contain a stacktrace by default, as that\u0026#8217;d only increase the size of Flight Recordings without adding much value   3 One payload attribute is defined for each relevant property such as HTTP method, media type, the invoked path etc.   4 @DataAmount tags this attribute as a data amount (by default in bytes) and will be displayed accordingly in JMC; there are many other similar annotations in the jdk.jfr package, such as @MemoryAddress, @Timestamp and more    Having defined the event class itself, we must find a way for emitting event instances at the right point in time. In the simplest case, e.g. suitable for events related to your application logic, this might happen right in the application code itself. For more \"technical\" events it\u0026#8217;s a good idea though to keep the creation of Flight Recorder events separate from your business logic, e.g. by using mechanisms such as servlet filters, interceptors and similar, which allow to inject cross-cutting logic into the call flow of your application.\n You also might employ byte code instrumentation at build or runtime for this purpose. The JMC Agent project aims at providing a configurable Java agent that allows to dynamically inject code for emitting JFR events into running programs. Via the EventFactory class, the JFR API also provides a way for defining event types dynamically, should their payload attributes only be known at runtime.\n For monitoring a JAX-RS based REST API, the ContainerRequestFilter and ContainerResponseFilter contracts come in handy, as they allow to hook into the request handling logic before and after a REST request gets processed:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 @Provider (1) public class FlightRecorderFilter implements ContainerRequestFilter, ContainerResponseFilter { @Override (2) public void filter(ContainerRequestContext requestContext) throws IOException { JaxRsInvocationEvent event = new JaxRsInvocationEvent(); if (!event.isEnabled()) { (3) return; } event.begin(); (4) requestContext.setProperty(JaxRsInvocationEvent.NAME, event); (5) } @Override (6) public void filter(ContainerRequestContext requestContext, ContainerResponseContext responseContext) throws IOException { JaxRsInvocationEvent event = (JaxRsInvocationEvent) requestContext .getProperty(JaxRsInvocationEvent.NAME); if (event == null || !event.isEnabled()) { return; } event.end(); (7) event.path = String.valueOf(requestContext.getUriInfo().getPath()); if (event.shouldCommit()) { (8) event.method = requestContext.getMethod(); event.mediaType = String.valueOf(requestContext.getMediaType()); event.length = requestContext.getLength(); event.queryParameters = requestContext.getUriInfo() .getQueryParameters().toString(); event.headers = requestContext.getHeaders().toString(); event.javaMethod = getJavaMethod(requestContext); event.responseLength = responseContext.getLength(); event.responseHeaders = responseContext.getHeaders().toString(); event.status = responseContext.getStatus(); event.commit(); (9) } } private String getJavaMethod(ContainerRequestContext requestContext) { String propName = \"org.jboss.resteasy.core.ResourceMethodInvoker\"; ResourceMethodInvoker invoker = (ResourceMethodInvoker)requestContext.getProperty(propName); return invoker.getMethod().toString(); } }      1 Allows the filter to be picked up automatically by the JAX-RS implementation   2 Will be invoked before the request is processed   3 Nothing to do if the event type is not enabled for recordings currently   4 Begin the timing of the event   5 Store the event in the request context, so it can be obtained again later on   6 Will be invoked after the request has been processed   7 End the timing of the event   8 The event should be committed if it is enabled and its duration is within the threshold configured for it; in that case, populate all the payload attributes of the event based on the values from the request and response contexts   9 Commit the event with Flight Recorder    With that, our event class is pretty much ready to be used. There\u0026#8217;s only one more thing to do, and that is registering the new type with the Flight Recorder system. A Quarkus application start-up lifecycle method comes in handy for that:\n 1 2 3 4 5 6 7 @ApplicationScoped public class Metrics { public void registerEvent(@Observes StartupEvent se) { FlightRecorder.register(JaxRsInvocationEvent.class); } }    Note this step isn\u0026#8217;t strictly needed, the event type can also be used without explicit registration. But doing so will later on allow to apply specific settings for the event in Mission Control (see below), also if no event of this type has been emitted yet.\n   Creating JFR Recordings Now let\u0026#8217;s capture some JAX-RS API events using Flight Recorder and inspect them in Mission Control.\n To do so, make sure to have Mission Control installed. Just as with OpenJDK, there are different builds for Mission Control to choose from. If you\u0026#8217;re in the Fedora/RHEL universe, there\u0026#8217;s a repository package which you can install, e.g. like this for the Fedora JMC package:\n 1 sudo dnf module install jmc:7/default    Alternatively, you can download builds for different platforms from Oracle; some more info about these builds can be found in this blog post by Marcus Hirt. There\u0026#8217;s also the Liberica Mission Control build by BellSoft and Zulu Mission Control by Azul. The AdoptOpenJDK provides snapshot builds of JMC 8 as well as an Eclipse update site for installing JMC into an existing Eclipse instance.\n If you\u0026#8217;d like to follow along and run these steps yourself, check out the source code from GitHub and then perform the following commands:\n 1 2 cd example-service \u0026amp;\u0026amp; mvn clean package \u0026amp;\u0026amp; cd .. docker-compose up --build    This builds the project using Maven and spins up the following services using Docker Compose:\n   example-service: The Todo example application\n  todo-db: The Postgres database used by the Todo service\n  prometheus and grafana: For monitoring live events later on\n   Then go to http://localhost:8080/todo, where you should see the Todo web application:\n   Now fire up Mission Control. The example service run via Docker Compose is configured so you can connect to it on localhost. In the JVM Browser, create a new connection with host \"localhost\" and port \"1898\". Hit \"Test connection\", which should yield \"OK\", then click \"Finish\".\n   Create a new recording by expanding the localhost:1898 node in the JVM Explorer, right-clicking on \"Flight Recorder\" and choosing \"Start Flight Recording\u0026#8230;\u0026#8203;\". Confirm the default settings, which will create a recording with a duration of one minute. Go back to the Todo web application and perform a few tasks like creating some new todos, editing and deleting them, or filtering the todo list.\n Either wait for the recording to complete or stop it by right-clicking on the recording name and selecting \"Stop\". Once the recording is done, it will be opened automatically. Now you could dive into all the logged events for the OS, the JVM etc, but as we\u0026#8217;re interested in our custom JAX-RS events, Choose \"Event Browser\" in the outline view and expand the \"JAX-RS\" category. You will see the events for all your REST API invocations, including information such as duration of the request, the HTTP method, the resource path and much more:\n   In a real-world use case, you could now use this information for instance to identify long-running requests and correlate these events with other data points in the Flight Recording, such as method profiling and memory allocation data, or sub-optimal SQL statements in your database.\n     If your application is running in production, it might not be feasible to connect to it via Mission Control from your local workstation. The jcmd utility comes in handy in that case; part of the JDK, you can use it to issue diagnostic commands against a running JVM.\n Amongst many other things, it allows you to start and stop Flight Recordings. On the environment with your running application, first run jcmd -l, which will show you the PIDs of all running Java processes. Having identified the PID of the process you\u0026#8217;d like to examine, you can initiate a recording like so:\n 1 2 jcmd \u0026lt;PID\u0026gt; JFR.start delay=5s duration=30s \\ name=MyRecording filename=my-recording.jfr    This will start a recording of 30 seconds, beginning in 5 seconds from now. Once the recording is done, you could copy the file to your local machine and load it into Mission Control for further analysis. To learn more about creating Flight Recordings via jcmd, refer to this great cheat sheet.\n     Another useful tool in the belt is the jfr command, which was introduced in JDK 12. It allows you to filter and examine the binary Flight Recording files. You also can use it to extract parts of a recording and convert them to JSON, allowing them to be processed with other tools. E.g. you could convert all the JAX-RS events to JSON like so:\n 1 jfr print --json --categories JAX-RS my-recording.jfr      Event Settings Sometimes it\u0026#8217;s desirable to configure detailed behaviors of a given event type. For the JAX-RS invocation event it might for instance make sense to only log invocations of particular paths in a specific recording, allowing for a smaller recording size and keeping the focus on a particular subset of all invocations. JFR supports this by the notion of event settings. Such settings can be specified when creating a recording; based on the active settings, particular events will be included or excluded in the recording.\n Inspired by the JavaDoc of @SettingDefinition let\u0026#8217;s see what\u0026#8217;s needed to enhance JaxRsInvocationEvent with that capability. The first step is to define a subclass of jdk.jfr.SettingControl, which serves as the value holder for our setting:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 public class PathFilterControl extends SettingControl { private Pattern pattern = Pattern.compile(\".*\"); (1) @Override (2) public void setValue(String value) { this.pattern = Pattern.compile(value); } @Override (3) public String combine(Set\u0026lt;String\u0026gt; values) { return String.join(\"|\", values); } @Override (4) public String getValue() { return pattern.toString(); } (5) public boolean matches(String s) { return pattern.matcher(s).matches(); } }      1 A regular expression pattern that\u0026#8217;ll be matched against the path of incoming events; by default all paths are included (.*)   2 Invoked by the JFR runtime to set the value for this setting   3 Invoked when multiple recordings are running at the same time, combining the settings values   4 Invoked by the runtime for instance when getting the default value of the setting   5 Matches the configured setting value against a particular path    On the event class itself a method with the following characteristics must be declared which will receive the setting by the JFR runtime:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 class JaxRsInvocationEvent extends Event { @Label(\"Path\") public String path; // other members... @Label(\"Path Filter\") @SettingDefinition (1) protected boolean pathFilter(PathFilterControl pathFilter) { (2) return pathFilter.matches(path); } }      1 Tags this as a setting   2 The method must be public, take a SettingControl type as its single parameter and return boolean    This method will be invoked by the JFR runtime during the shouldCommit() call. It passes in the setting value of the current recording so it can applied to the path value of the given event. In case the filter returns true, the event will be added to the recording, otherwise it will be ignored.\n We also could use such setting to control the inclusion or exclusion of specific event attributes. For that, the setting definition method would always have to return true, but depending on the actual setting it might set particular attributes of the event class to null. For instance this might come in handy if we wanted to log the entire request/response body of our REST API. Doing this all the time might be prohibitive in terms of recording size, but it might be enabled for a particlar short-term recording for analyzing some bug.\n Now let\u0026#8217;s see how the path filter can be applied when creating a new recording in Mission Control. The option is a bit hidden, but here\u0026#8217;s how you can enable it. First, create a new Flight Recording, then choose \"Template Manager\" in the dialogue:\n   Duplicate the \"Continuous\" template and edit it:\n   Click \"Advanced\":\n   Expand \"JAX-RS\" \u0026#8594; \"JAX-RS Invocation\" and put .*(new|edit).* into the Path Filter control:\n   Now close the last two dialogues. In the \"Start Flight Recording\" dialogue make sure to select your new template under \"Event Settings\"; although you\u0026#8217;ve edited it before, it won\u0026#8217;t be selected automatically. I lost an hour or so wondering why my settings were not applied\u0026#8230;\u0026#8203; .\n Lastly, click \"Finish\" to begin the recording:\n   Perform some tasks in the Todo web app and stop the recording. You should see only the REST API calls for the new and edit operations, whereas no events should be shown for the list and delete operations of the API.\n     In order to apply specific settings when creating a recording on the CLI using jcmd, edit the settings as described above. Then go to the Template Manager and export the profile you\u0026#8217;d like to use. When starting the recording via jcmd, specify the settings file via the settings=/path/to/settings.jfc parameter.\n       JFR Event Streaming Flight Recorder files are great for analyzing performance characteristics in an \"offline\" approach: you can take recordings in your production environment and ship them to your work station or a remote support team, without requiring live access to the running application. This is also an interesting mode for open-source projects, where maintainers typically don\u0026#8217;t have access to running applications of their users. Exchanging Flight Recordings (limited to a sensible subset of information, so to avoid exposure of confidential internals) might allow open source developers to gain insight into characteristics of their libraries when deployed to production at their users.\n But there\u0026#8217;s another category of use cases for event data sourced from applications, the JVM and the operating system, where the recording file approach doesn\u0026#8217;t quite fit: live monitoring and alerting of running applications. E.g. operations teams might want to set up dashboards showing the most relevant application metrics in \"real-time\", without having to create any recording files first. A related requirement is alerting, so to be notified when metrics reach a certain threshold. For instance it might be desirable to be alterted if the request duration of our JAX-RS API goes beyond a defined value such as 100 ms.\n This is where JEP 349 (\"JFR Event Streaming\") comes in. It\u0026#8217;ll be part of Java 14 and its stated goal is to \"provide an API for the continuous consumption of JFR data on disk, both for in-process and out-of-process applications\". That\u0026#8217;s exactly what we need for our monitoring/dashboarding use case. Using the Streaming API, Flight Recorder events of the running application can be exposed to external consumers, without having to explicitly load any recording files.\n Now it may be prohibitively expensive to stream each and every event with all its detailed information to remote clients. But that\u0026#8217;s not needed for monitoring purposes anyways. Instead, we can expose metrics based on our events, such as the total number and frequency of REST API invocations, or the average and 99th percentile duration of the calls.\n   MicroProfile Metrics The following shows a basic implementation of exposing these metrics for the JAX-RS API events to Prometheus/Grafana, where they can be visualized using a dashboard. Being based on Quarkus, the Todo web application can leverage all the MicroProfile APIs. On of them is the MicroProfile Metrics API, which defines a \"unified way for Microprofile servers to export Monitoring data (\"Telemetry\") to management agents\".\n While the MicroProfile Metrics API is used in an annotation-driven fashion often-times, it also provides a programmatic API for registering metrics. This can be leveraged to expose metrics based on the JAX-RS Flight Recorder events:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 @ApplicationScoped public class Metrics { @Inject (1) MetricRegistry metricsRegistry; private RecordingStream recordingStream; (2) public void onStartup(@Observes StartupEvent se) { recordingStream = new RecordingStream(); (3) recordingStream.enable(JaxRsInvocationEvent.NAME); recordingStream.onEvent(JaxRsInvocationEvent.NAME, event -\u0026gt; { (4) String path = event.getString(\"path\") .replaceAll(\"(\\\\/)([0-9]+)(\\\\/?)\", \"$1{param}$3\"); (5) String method = event.getString(\"method\"); String name = path + \"-\" + method; Metadata metadata = metricsRegistry.getMetadata().get(name); if (metadata == null) { metricsRegistry.timer(Metadata.builder() (6) .withName(name) .withType(MetricType.TIMER) .withDescription(\"Metrics for \" + path + \" (\" + method + \")\") .build()).update(event.getDuration().toNanos(), TimeUnit.NANOSECONDS); } else { (7) metricsRegistry.timer(name).update(event.getDuration() .toNanos(), TimeUnit.NANOSECONDS); } }); recordingStream.startAsync(); (8) } public void stop(@Observes ShutdownEvent se) { recordingStream.close(); (9) try { recordingStream.awaitTermination(); } catch (InterruptedException e) { throw new RuntimeException(e); } } }      1 Inject the MicroProfile Metrics registry   2 A stream providing push access to JFR events   3 Initialize the stream upon application start-up, so it includes the JAX-RS invocation events   4 For each JaxRsInvocationEvent this callback will be invoked   5 To register a corresponding metric, any path parameters are replaced with a constant placeholder, so that e.g. all invocations of the todo/{id}/edit path are exposed via one single metric instead of having separate ones for Todo 1, Todo 2 etc.   6 If the metric for the specific path hasn\u0026#8217;t been registered yet, then do so; it\u0026#8217;s a metric of type TIMER, allowing metric consumers to track the duration of calls of that particular path   7 If the metric for the path has been registered before, update its value with the duration of the incoming event   8 Start the stream asynchronously, not blocking the onStartup() method   9 Close the JFR event stream upon application shutdown    When connecting to the running application using JMC now, you\u0026#8217;ll see a continuous recording, which serves as the basis for the event stream. It only contains events of the JaxRsInvocationEvent type.\n MicroProfile Metrics exposes any application-provided metrics in the Prometheus format under the /metrics/application endpoint; for each operation of the REST API, e.g. POST to /todo/{id}/edit, the following metrics are provided:\n   request rate per second, minute, five minutes and 15 minutes\n  min, mean and max duration as well as standard deviation\n  total invocation count\n  duration of 75th, 95th, 99th etc. percentiles\n     Once the endpoint is provided, it\u0026#8217;s not difficult to set up a scraping process for ingesting the metrics into the Prometheus time-series database. You can find the required Prometheus configuration in the accompanying source code repository.\n While Prometheus provides some visualization capabilities itself, it is often used together with Grafana, which allows to build nicely looking dashboards via a rather intuitive UI. Here\u0026#8217;s an example dashboard showing the duration and invocation numbers for the different methods in the Todo REST API:\n   Again you can find the complete configuration for Grafana including the definition of that dashboard in the example repo. It will automatically be loaded when using the Docker Compose set-up shown above. Based on that you could easily expand the dashboard for other metrics and set up alerts, too.\n Combining the monitoring of live key metrics with the deep insights possible via detailed JFR recordings enable a very powerful workflow for analysing performance issues in production:\n   When setting up the continuous recording that serves as the basis for the metrics, have it contain all the event types you\u0026#8217;d need to gain insight into GC or memory issues etc.; specify a maximum size via RecordingStream#setMaxSize(), so to avoid an indefinitely growing recording; you\u0026#8217;ll probably need to experiment a bit to find the right trade-off between number of enabled events, duration that\u0026#8217;ll be covered by the recording and the required disk space\n  Only expose a relevant subset of the events as metrics to Prometheus/Grafana, such as the JAX-RS API invocation events in our example\n  Set up an alert in Grafana on the key metrics, e.g. mean duration of the REST calls, or 99th percentile thereof\n  If the alert triggers, take a dump of the last N minutes of the continuous recording via JMC or jcmd (using the JFR.dump command), and analyze that detailed recording to understand what was happening in the time leading to the alert\n     Summary and Related Work Flight Recorder and Mission Control are excellent tools providing deep insight into the performance characteristics of Java applications. While there\u0026#8217;s a large amount of data and highly valuable information provided out the box, JFR and JMC also allow for the recording of custom, application-specific events. With its low overhead, JFR can be enabled on a permanent basis in production environments. Combined with the Event Streaming API introduced in Java 14, this opens up an attractive, very performant alternative to other means of capturing analysis information at application runtime, such as logging libraries. Providing live key metrics derived from JFR events to tools such as Prometheus and Grafana enables monitoring and alerting in \"real-time\".\n For many enterprises that are still on Java 11 or even 8, it\u0026#8217;ll still be far out into the future until they might adopt the streaming API. But with more and more companies joining the OpenJDK efforts, it might be a possiblity that this useful feature gets backported to earlier LTS releases, just as the open-sourced version of Flight Recorder itself got backported to Java 8.\n There are quite a few posts and presentations about JFR and JMC available online, but many of them refer to older versions of those tools, before they got open-sourced. Here are some up-to-date resources which I found very helpful:\n   Continuous Monitoring with JDK Flight Recorder: a talk from QCon SF 2019 by Mikael Vidstedt\n  Flight Recorder \u0026amp; Mission Control at Code One 2019: a compilation of several great sessions on these two tools at last year\u0026#8217;s Code One, put together by Marcus Hirt\n  Digging Into Sockets With Java Flight Recorder: blog post by Petr Bouda on identifying performance bottlenecks with JFR in a Netty-based web application\n   Lastly, the Red Hat OpenJDK team is working on some very interesting projects around JFR and JMC, too. E.g. they\u0026#8217;ve built a datasource for Grafana which lets you examine the events of a JFR file. They also work on tooling to simplify the usage of JFR in container-based environments such as Kubernetes and OpenShift, including a K8s Operator for controlling Flight Recordings and a web-based UI for managing JFR in remote JVMs. Should you happen to be at the FOSDEM conference in Brussels on the next weekend, be sure to not miss the JMC \u0026amp; JFR - 2020 Vision session by Red Hat engineer Jie Kang.\n If you\u0026#8217;d like to experiment with JDK Flight Recorder and JDK Mission Control based on the Todo web application yourself, you can find the complete source code for this post on GitHub.\n Many thanks to Mario Torre and Jie Kang for reviewing an early draft of this post.\n  ","id":8,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eThe \u003ca href=\"https://openjdk.java.net/jeps/328\"\u003eJDK Flight Recorder\u003c/a\u003e (JFR) is an invaluable tool for gaining deep insights into the performance characteristics of Java applications.\nOpen-sourced in JDK 11, JFR provides a low-overhead framework for collecting events from Java applications, the JVM and the operating system.\u003c/p\u003e\n\u003c/div\u003e\n\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIn this blog post we\u0026#8217;re going to explore how custom, application-specific JFR events can be used to monitor a REST API, allowing to track request counts, identify long-running requests and more.\nWe\u0026#8217;ll also discuss how the JFR \u003ca href=\"https://openjdk.java.net/jeps/349\"\u003eEvent Streaming API\u003c/a\u003e new in Java 14 can be used to export live events,\nmaking them available for monitoring and alerting via tools such as Prometheus and Grafana.\u003c/p\u003e\n\u003c/div\u003e","tags":["java","monitoring","microprofile","jakartaee","quarkus"],"title":"Monitoring REST APIs with Custom JDK Flight Recorder Events","uri":"http://localhost:1313/blog/rest-api-monitoring-with-custom-jdk-flight-recorder-events/","year":"2020"},{"content":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.\n Record Invariants and Bean Validation Records (a preview feature as of Java 14) help to cut down the ceremony when defining plain data holder objects. In a nutshell, you solely need to declare the attributes that should make up the state of the record type (\"components\" in terms of JEP 359), and quite a few things you\u0026#8217;d otherwise have to implement by hand will be created for you automatically:\n   a private final field and a corresponding read accessor for each component\n  a constructor for passing in all component values\n  toString(), equals() and hashCode() methods.\n   As an example, here\u0026#8217;s a record Car with three components:\n 1 2 3 public record Car(String manufacturer, String licensePlate, int seatCount) { }    Now let\u0026#8217;s assume a few class invariants should be applied to this record (inspired by an example from the Hibernate Validator reference guide):\n   manufacturer is a non-blank string\n  license plate is never null and has a length of 2 to 14 characters\n  seatCount is at least 2\n   Class invariants like these are specific conditions or rules applying to the state of a class (as manifesting in its fields), which always are guaranteed to be satisfied for the lifetime of an instance of the class.\n The Bean Validation API defines a way for expressing and validating constraints using Java annotations. By putting constraint annotations to the components of a record type, it\u0026#8217;s a perfect means of describing the invariants from above:\n 1 2 3 4 5 public record Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { }    Of course declaring constraints using annotations by itself won\u0026#8217;t magically enforce these invariants. In order to do so, the javax.validation.Validator API must be invoked at suitable points in the object lifecycle, so to avoid any of the invariants to be violated. As records are immutable, it is sufficient to validate the constraints once when creating a new Car instance. If no constraints are violated, the created instance is guaranteed to always satisfy its invariants.\n   Implementation The key question now is how to validate the invariants while constructing new Car instances. This is where Bean Validation\u0026#8217;s API for method validation comes in: it allows to validate pre- and post-conditions that should be satisfied when a Java method or constructor gets invoked. Pre-conditions are expressed by applying constraints to method and constructor parameters, whereas post-conditions are expressed by putting constraints to a method or constructor itself.\n This can be leveraged for enforcing record invariants: as it turns out, any annotations on the components of a record type are also copied to the corresponding parameters of the generated constructor. I.e. the Car record implicitly has a constructor which looks like this:\n 1 2 3 4 5 6 7 8 9 public Car( @NotBlank String manufacturer, @NotNull @Size(min = 2, max = 14) String licensePlate, @Min(2) int seatCount) { this.manufacturer = manufacturer; this.licensePlate = licensePlate; this.seatCount = seatCount; }    That\u0026#8217;s exactly what we need: by validating these parameter constraints upon instantiation of the Car class, we can make sure that only valid objects can ever be created, ensuring that the record type\u0026#8217;s invariants are always guaranteed.\n What\u0026#8217;s missing is a way for automatically validating them upon constructor invocation. The idea for that is to enhance the byte code of the implicit Car constructor so that it passes the incoming parameter values to Bean Validation\u0026#8217;s ExecutableValidator#validateConstructorParameters() method and raises a constraint violation exception in case of any invalid parameter values.\n We\u0026#8217;re going to use the excellent ByteBuddy library for this job. Here\u0026#8217;s a slightly simplified implementation for invoking the executable validator (you can find the complete source code of this example in this GitHub repository):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 public class ValidationInterceptor { private static final Validator validator = Validation (1) .buildDefaultValidatorFactory() .getValidator(); public static \u0026lt;T\u0026gt; void validate(@Origin Constructor\u0026lt;T\u0026gt; constructor, @AllArguments Object[] args) { (2) Set\u0026lt;ConstraintViolation\u0026lt;T\u0026gt;\u0026gt; violations = validator (3) .forExecutables() .validateConstructorParameters(constructor, args); if (!violations.isEmpty()) { String message = violations.stream() (4) .sorted(ValidationInterceptor::compare) .map(cv -\u0026gt; getParameterName(cv) + \" - \" + cv.getMessage()) .collect(Collectors.joining(System.lineSeparator())); throw new ConstraintViolationException( (5) \"Invalid instantiation of record type \" + constructor.getDeclaringClass().getSimpleName() + System.lineSeparator() + message, violations); } } private static int compare(ConstraintViolation\u0026lt;?\u0026gt; o1, ConstraintViolation\u0026lt;?\u0026gt; o2) { return Integer.compare(getParameterIndex(o1), getParameterIndex(o2)); } private static String getParameterName(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter name } private static int getParameterIndex(ConstraintViolation\u0026lt;?\u0026gt; cv) { // traverse property path to extract parameter index } }      1 Obtain a Bean Validation Validator instance   2 The @Origin and @AllArguments annotations are the hint to ByteBuddy that the invoked constructor and parameter values should be passed to this method from within the enhanced constructor   3 Validate the passed constructor arguments using Bean Validation   4 If there\u0026#8217;s at least one violated constraint, create a message comprising all constraint violation messages, ordered by parameter index   5 Raise a ConstraintViolationException, containing the message created before as well as all the constraint violations    Having implemented the validation interceptor, the code of the record constructor must be enhanced by ByteBuddy, so that it invokes the inceptor. ByteBuddy provides different ways for doing so, e.g. at application start-up using a Java agent. For this example, we\u0026#8217;re going to employ build-time enhancement via the ByteBuddy Maven plug-in. The enhancement logic itself is implemented in a custom net.bytebuddy.build.Plugin:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class ValidationWeavingPlugin implements Plugin { @Override public boolean matches(TypeDescription target) { (1) return target.getDeclaredMethods() .stream() .anyMatch(m -\u0026gt; m.isConstructor() \u0026amp;\u0026amp; hasConstrainedParameter(m)); } @Override public Builder\u0026lt;?\u0026gt; apply(Builder\u0026lt;?\u0026gt; builder, TypeDescription typeDescription, ClassFileLocator classFileLocator) { return builder.constructor(this::hasConstrainedParameter) (2) .intercept(SuperMethodCall.INSTANCE.andThen( MethodDelegation.to(ValidationInterceptor.class))); } private boolean hasConstrainedParameter(MethodDescription method) { return method.getParameters() (3) .asDefined() .stream() .anyMatch(p -\u0026gt; isConstrained(p)); } private boolean isConstrained( ParameterDescription.InDefinedShape parameter) { (4) return !parameter.getDeclaredAnnotations() .asTypeList() .filter(hasAnnotation(annotationType(Constraint.class))) .isEmpty(); } @Override public void close() throws IOException { } }      1 Determines whether a type should be enhanced or not; this is the case if there\u0026#8217;s at least one constructor that has one more more constrained parameters   2 Applies the actual enhancement: into each constrained constructor the call to ValidationInterceptor gets injected   3 Determines whether a method or constructor has at least one constrained parameter   4 Determines whether a parameter has at least one constraint annotation (an annotation meta-annotated with @Constraint; for the sake of simplicity the case of constraint inheritance is ignored here)    The next step is to configure the ByteBuddy Maven plug-in in the pom.xml of the project:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;net.bytebuddy\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;byte-buddy-maven-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${version.bytebuddy}\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;transform\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformations\u0026gt; \u0026lt;transformation\u0026gt; \u0026lt;plugin\u0026gt; dev.morling.demos.recordvalidation.implementation.ValidationWeavingPlugin \u0026lt;/plugin\u0026gt; \u0026lt;/transformation\u0026gt; \u0026lt;/transformations\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt;    This plug-in runs in the process-classes phase by default, so it can access and enhance the class files generated during compilation. If you were to build the project now, you could use the javap tool to examine the byte code of the Car class,and you\u0026#8217;d see that the implicit constructor of that class contains an invocation of the ValidationInterceptor#validate() method.\n As an example, let\u0026#8217;s consider the following attempt to instantiate a Car object, which violates the invariants of that record type:\n 1 Car invalid = new Car(\"\", \"HH-AB-123\", 1);    A constraint violation like this will be thrown immediately:\n 1 2 3 4 5 javax.validation.ConstraintViolationException: Invalid instantiation of record type Car manufacturer - must not be blank seatCount - must be greater than or equal to 2 at dev.morling.demos.recordvalidation.RecordValidationTest.canValidate(RecordValidationTest.java:20)    If all constraints are satisfied, no exception will be thrown and the caller obtains the new Car instance, whose invariants are guaranteed to be met for the remainder of the object\u0026#8217;s lifetime.\n   Advantages Having shown how Bean Validation can be leveraged to enforce the invariants of Java record types, it is time to reflect: is this this approach worth the additional complexity incurred by adding a library such as Bean Validation and hooking it up using byte code enhancement? After all, you could also validate incoming parameter values using methods such as Objects#requireNonNull().\n As so often, you need to make such decision based on your specific requirements and needs. Here are some advantages I can see about the Bean Validation approach:\n   Invariants become part of the API: Constraint annotations on public API members such as the implicit record constructor are easily discoverable by users of such type; they are listed in generated JavaDoc, you can see them when hovering over an invocation in your IDE (once records are supported); when used on the DTOs of a REST layer, the invariants could also be added to automatically generated API documentation. All this makes it easy for users of the type to understand the invariants and also avoids potential inconsistencies between a manual validation implementation and corresponding hand-written documentation\n  Providing constraint metadata: The Bean Validation constraint meta-data API can be used to obtain information about the constraints of Java types; for instance this can be used to implement client-side validation of constraints in a web application\n  Less code: Putting constraint annotations directly to the record components themselves avoids the need for implementing these checks manually in an explicit canonical constructor\n  I18N support: Bean Validation provides means of internationalizing constraint violation messages; if your record types are instantiated based on user input (e.g. when using them as data types in a REST API), this allows for localized error messages in the UI\n  Returning all constraints at once: For UIs it\u0026#8217;s typically beneficial to return all the constraint violations at once instead of showing them one by one; while doable in a hand-written implementation, it requires a bit of effort, whereas you get this \"for free\" when using Bean Validation which always returns a set of all the violations\n  Lots of ready-made constraints: Bean Validation comes with a range of constraints out of the box; in addition libraries such as Hibernate Validator and others provide many more ready-to-use constraints, coming in handy for instance when implementing domain-specific value types with complex validation rules:\n1 2 3 public record EmailAddress( @Email @NotNull @Size(min=1, max=250) String value) { }      Support for validation groups: Bean Validation\u0026#8217;s concept of validation groups allows you to validate only sub-sets of constraints in specific contexts; e.g. based on location and applying legal requirements\n  Dynamic constraint definition: Using Hibernate Validator, constraints can also be declared dynamically using a fluent API. This can be very useful when your validation requirements vary at runtime, e.g. if you need to apply different constraint configurations for different tenants.\n     Limitations One area where this current proof-of-concept implementation falls a bit short is the validation of invariants that apply to multiple components. For instance consider a record type representing an interval with a begin and an end attribute, where you\u0026#8217;d like to enforce the invariant that end is larger than begin.\n Bean Validation addresses this sort of requirement via class-level constraints and, for method and constructor validation, cross-parameter constraints. Class-level constraints are not really suitable for our purposes, because we want to validate the invariants before an object instance is created.\n Cross-parameter constraints on the other hand are exactly what we\u0026#8217;d need. As they must be given on a constructor or method, the canonical constructor of a record must be explicitly declared in this case. Using Hibernate Validator\u0026#8217;s @ParameterScriptAssert constraint, the invariant from above could be expressed like so:\n 1 2 3 4 5 6 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval { } }    This works as expected, but there\u0026#8217;s one caveat: any annotations from the record components are not propagated to the corresponding parameters of the canoncial constructor in this case. This means that any constraints given on the individual components would be lost. Right now it\u0026#8217;s not quite clear to me whether that\u0026#8217;s an intended behavior or rather a bug in the current record implementation.\n If indeed it is intentional, than there\u0026#8217;d be no way other than specifying the constraints explicitly on the parameters of a fully manually implemented constructor:\n 1 2 3 4 5 6 7 8 public record Interval(int begin, int end) { @ParameterScriptAssert(lang=\"javascript\", script=\"end \u0026gt; begin\") public Interval(@Positive int begin, @Positive int end) { this.begin = begin; this.end = end; } }    This works, but of course we\u0026#8217;re losing a bit of the conciseness promised by records.\n Update, Jan 20, 2020, 20:57: Turns out, the current behavior indeed is not intended (see JDK-8236597) and in a future Java version the shorter version of the code shown above should work.\n   Wrap-Up In this blog post we\u0026#8217;ve explored how invariants on Java 14 record types can be enforced using the Bean Validation API. With just a bit of byte code magic the task gets manageable: by validating invariants expressed by constraint annotations on record components right at instantiation time, only valid record instances will ever be exposed to callers. Key for that is the fact that any annotations from record components are automatically propagated to the corresponding parameters of the canonical record constructor. That way they can be validated using Bean Validation\u0026#8217;s method validation API. It remains to be seen, whether invariants based on multiple record components also can be enforced as easily.\n From the perspective of the Bean Validation specification, it\u0026#8217;ll surely make sense to explore support for record types. While not as powerful as enforcing invariants at construction time via byte code enhancement, it might also be useful to support the validation of component values via their read accessors. For that, the notion of \"properties\" would have to be relaxed, as the read accessors of records don\u0026#8217;t have the JavaBeans get prefix currently expected by Bean Validation. It also should be considered to expand the Bean Validation metadata API accordingly.\n I would also be very happy to learn about your thoughts around this topic. While Bean Validation 3.0 (as part of Jakarta EE 9) in all likelyhood won\u0026#8217;t bring any changes besides the transition to the jakarta.* package namespace, this may be an area where we could evolve the specification for Jakarta EE 10.\n If you\u0026#8217;d like to experiment with the validation of record types yourself, you can find the complete source code on GitHub.\n   ","id":9,"section":"blog","summary":"Record types are one of the most awaited features in Java 14; they promise to \"provide a compact syntax for declaring classes which are transparent holders for shallowly immutable data\". One example where records should be beneficial are data transfer objects (DTOs), as e.g. found in the remoting layer of enterprise applications. Typically, certain rules should be applied to the attributes of such DTO, e.g. in terms of allowed values. The goal of this blog post is to explore how such invariants can be enforced on record types, using annotation-based constraints as provided by the Bean Validation API.","tags":["bean-validation","jakartaee"],"title":"Enforcing Java Record Invariants With Bean Validation","uri":"http://localhost:1313/blog/enforcing-java-record-invariants-with-bean-validation/","year":"2020"},{"content":"When Java 9 was introduced in 2017, it was the last major version published under the old release scheme. Since then, a six month release cadence has been adopted. This means developers don\u0026#8217;t have to wait years for new APIs and language features, but they can get their hands onto the latest additions twice a year. In this post I\u0026#8217;d like to describe how you can try out new language features such as Java 13 text blocks in the test code of your project, while keeping your main code still compatible with older Java versions.\n One goal of the increased release cadence is to shorten the feedback loop for the OpenJDK team: have developers in the field try out new functionality early on, collect feedback based on that, adjust as needed. To aid with that process, the JDK has two means of publishing preliminary work before new APIs and language features are cast in stone:\n   Incubator JDK modules\n  Preview language and VM features\n   An example for the former is the new HTTP client API, which was an incubator module in JDK 9 and 10, before it got standardized as a regular API in JDK 11. Examples for preview language features are switch expressions (added as a preview feature in Java 12) and text blocks (added in Java 13).\n Now especially text blocks are a feature which many developers have missed in Java for a long time. They are really useful when embedding other languages, or just any kind of longer text into your Java program, e.g. multi-line SQL statements, JSON documents and others. So you might want to go and use them as quickly as possible, but depending on your specific situation and requirements, you may no be able to move to Java 13 just yet.\n In particular when working on libraries, compatibility with older Java versions is a high priority in order to not cut off a large number of potential users. E.g. in the JetBrains Developer Ecosystem Survey from early 2019, 83% of participants said that Java 8 is a version they regularly use. This matches with what I\u0026#8217;ve observed myself during conversations e.g. at conferences. Now this share may have reduced a bit since then (I couldn\u0026#8217;t find any newer numbers), but at this point in time it still seems save to say that libraries should support Java 8 to not limit their audience in a signficant way.\n So while building on Java 13 is fine, requiring it at runtime for libraries isn\u0026#8217;t. Does this mean as a library author you cannot use text blocks then for many years to come? For your main code (i.e. the one shipped to users) it indeed does mean that, but things look different when it comes to test code.\n An Example One case where text blocks come in extremely handy is testing of REST APIs, where JSON requests need to created and responses may have to be compared to a JSON string with the expected value. Here\u0026#8217;s an example of using text blocks in a test of a Quarkus-based REST service, implemented using RESTAssured and JSONAssert:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 @QuarkusTest public class TodoResourceTest { @Test public void canPostNewTodoAndReceiveId() throws Exception { given() .when() .body(\"\"\" (1) { \"title\" : \"Learn Java\", \"completed\" : false } \"\"\" ) .contentType(ContentType.JSON) .post(\"/hello\") .then() .statusCode(201) .body(matchesJson(\"\"\" (2) { \"id\" : 1, \"title\" : \"Learn Java\", \"completed\" : false } \"\"\") ); } }      1 Text block with the JSON request to send   2 Text block with the expected JSON response    Indeed that\u0026#8217;s much nicer to read, e.g. when comparing the request JSON to the code you\u0026#8217;d typically write without text blocks. Concatenating multiple lines, escaping quotes and explicitly specifying line breaks make this quite cumbersome:\n 1 2 3 4 5 6 .body( \"{\\n\" + \" \\\"title\\\" : \\\"Learn Java 13\\\",\\n\" + \" \\\"completed\\\" : false\\n\" + \"}\" )    Now let\u0026#8217;s see what\u0026#8217;s needed in terms of configuration to enable usage of Java 13 text blocks for tests, while keeping the main code of a project compatible with Java 8.\n   Configuration Two options of the Java compiler javac come into play here:\n   --release: specifies the Java version to compile for\n  --enable-preview: allows to use language features currently in \"preview\" status such as text blocks as of Java 13/14\n       The --release option was introduced in Java 9 and should be preferred over the more widely known pair of --source and --target. The reason being that --release will prevent any accidental usage of APIs only introduced in later versions.\n E.g. say you were to write code such as List.of(\"Foo\", \"Bar\"); the of() methods on java.util.List were only introduced in Java 9, so compiling with --release 8 will raise a compilation error in this case. When using the older options, this situation wouldn\u0026#8217;t be detected at compile time, making the problem only apparent when actually running the application on the older Java version.\n     Build tools typically allow to use different configurations for the compilation of main and test code. E.g. here is what you\u0026#8217;d use for Maven (you can find the complete source code of the example in this GitHub repo):\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 ... \u0026lt;properties\u0026gt; ... \u0026lt;maven.compiler.release\u0026gt;8\u0026lt;/maven.compiler.release\u0026gt; (1) ... \u0026lt;/properties\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-compiler-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.8.1\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;default-testCompile\u0026lt;/id\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;release\u0026gt;13\u0026lt;/release\u0026gt; (2) \u0026lt;compilerArgs\u0026gt;--enable-preview\u0026lt;/compilerArgs\u0026gt; (3) \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ... \u0026lt;/plugins\u0026gt; ... \u0026lt;/build\u0026gt; ...      1 Compile for release 8 by default, i.e. the main code   2 Compile test code for release 13   3 Also pass the --enable-preview option when compiling the test code    Also at runtime preview features must be explicitly enabled. Therefore the java command must be accordingly configured when executing the tests, e.g. like so when using the Maven Surefire plug-in:\n 1 2 3 4 5 6 7 8 9 ... \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.22.1\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;argLine\u0026gt;--enable-preview\u0026lt;/argLine\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; ...    With this configuration in place, text blocks can now be used in tests as the one above, but not in the main code of the program. Doing so would result in a compilation error.\n Note your IDE might still let you do this kind of mistake. At least Eclipse chose for me the maximum of main (8) and test code (13) release levels when importing the project. But running the build on the command line via Maven or on your CI server will detect this situation.\n As Java 13 now is required to build this code base, it\u0026#8217;s a good idea to make this prerequisite explicit in the build process itself. The Maven enforcer plug-in comes in handy for that, allowing to express this requirement using its Java version rule:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 ... \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-enforcer-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;id\u0026gt;enforce-java\u0026lt;/id\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;enforce\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;requireJavaVersion\u0026gt; \u0026lt;version\u0026gt;[13,)\u0026lt;/version\u0026gt; \u0026lt;/requireJavaVersion\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; ...    The plug-in will fail the build when being run on a version before Java 13.\n   Should You Do This? Having seen how you can use preview features in test code, the question is: should you actually do this? A few things should be kept in mind for answering that. First of all, preview features are really that, a preview. This means that details may change in future Java revisions. Or, albeit unlikely, such feature may even be dropped altogether, should the JDK team arrive at the conclusion that it is fundamentally flawed.\n Another important factor is the minimum Java language version supported by the JDK compiler. As of Java 13, the oldest supported release is 7; i.e. using JDK 13, you can produce byte code that can be run with Java versions as old as Java 7. In order to keep the Java compiler maintainable, support for older versions is dropped every now and then. Right now, there\u0026#8217;s no formal process in place which would describe when support for a specific version is going to be removed (defining such policy is the goal of JEP 182).\n As per JDK developer Joe Darcy, \"there are no plans to remove support for --release 7 in JDK 15\". Conversely, this means that support for release 7 theoretically could be removed in JDK 16 and support for release 8 could be removed in JDK 17. In that case you\u0026#8217;d be caught between a rock and a hard place: Once you\u0026#8217;re on a non-LTS (\"long-term support\") release like JDK 13, you\u0026#8217;ll need to upgrade to JDK 14, 15 etc. as soon as they are out, in order to not be cut off from bug fixes and security patches. Now while doing so, you\u0026#8217;d be forced to increase the release level of your main code, once support for release 8 gets dropped, which may not desirable. Or you\u0026#8217;d have to apply some nice awk/sed magic to replace all those shiny text blocks with traditional concatenated and escaped strings, so you can go back to the current LTS release, Java 11. Not nice, but surely doable.\n That being said, this all doesn\u0026#8217;t seem like a likely scenario to me. JEP 182 expresses a desire \"that source code 10 or more years old should still be able to be compiled\"; hence I think it\u0026#8217;s save to assume that JDK 17 (the next release planned to receive long-term support) will still support release 8, which will be seven years old when 17 gets released as planned in September 2021. In that case you\u0026#8217;d be on the safe side, receiving update releases and being able to keep your main code Java 8 compatible for quite a few years to come.\n Needless to say, it\u0026#8217;s a call that you need to make, deciding for yourself wether the benefits of using new language features such as text blocks is worth it in your specific situation or not.\n  ","id":10,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eWhen Java 9 was introduced in 2017,\nit was the last major version published under the old release scheme.\nSince then, a \u003ca href=\"https://www.infoq.com/news/2017/09/Java6Month/\"\u003esix month release cadence\u003c/a\u003e has been adopted.\nThis means developers don\u0026#8217;t have to wait years for new APIs and language features,\nbut they can get their hands onto the latest additions twice a year.\nIn this post I\u0026#8217;d like to describe how you can try out new language features such as \u003ca href=\"http://openjdk.java.net/jeps/355\"\u003eJava 13 text blocks\u003c/a\u003e in the test code of your project,\nwhile keeping your main code still compatible with older Java versions.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Using Java 13 Text Blocks (Only) for Your Tests","uri":"http://localhost:1313/blog/using-java-13-text-blocks-for-tests/","year":"2020"},{"content":"One of the long-awaited features in Quarkus was support for server-side templating: until recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend. This has changed with Quarkus 1.1: it comes with a brand-new template engine named Qute, which allows to build web applications using server-side templates.\n When looking at frameworks for building web applications, there\u0026#8217;s two large categories:\n   client-side solutions based on JavaScript such as React, vue.js or Angular\n  server-side frameworks such as Spring Web MVC, JSF or MVC 1.0 (in the Java world)\n   Both have their indivdual strengths and weaknesses and it\u0026#8217;d be not very wise to always prefer one over the other. Instead, the choice should be based on specific requirements (e.g. what kind of interactivity is needed) and prerequisites (e.g. the skillset of the team building the application).\n Being mostly experienced with Java, server-side solutions are appealing to me, as they allow me to use the language I know and tooling (build tools, IDEs) I\u0026#8217;m familiar and most productive with. So when Qute was announced, it instantly caught my attention and I had to give it a test ride. In this post I want to share some of the experiences I made.\n Note this isn\u0026#8217;t a comprehensive tutorial for building web apps with Qute, instead, I\u0026#8217;d like to discuss a few things that stuck out to me. You can find a complete working example here on GitHub. It implements a basic CRUD application for managing personal todos, persisted in a Postgres database. Here\u0026#8217;s a video that shows the demo in action:\n    The Basics The Qute engine is based on RESTEasy/JAX-RS. As such, Qute web applications are implemented by defining resource types with methods answering to specific HTTP verbs and accept headers. The only difference being, that HTML pages are returned instead of JSON as in your typical REST-ful data API. The individual pages are created by processing template files. Here\u0026#8217;s a basic example for returning all the Todo records in our application:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 @Path(\"/todo\") public class TodoResource { @Inject Template todos; @GET (1) @Consumes(MediaType.TEXT_HTML) (2) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos() { return todos.data(\"todos\", Todo.findAll().list()); (3) } }      1 Processes HTTP GET requests for /todo   2 This method consumes and produces the text/html media type   3 Obtain all todos from the database and feed them to the todos template    The Todo class is as JPA entity implemented via Hibernate Panache:\n 1 2 3 4 5 6 7 @Entity public class Todo extends PanacheEntity { public String title; public int priority; public boolean completed; }    Panache is a perfect fit for this kind of CRUD applications. It helps with common tasks such as id mapping, and by means of the active record pattern you get query methods like findAll() \"for free\".\n To produce an HTML page for displaying the result list, the todos template is used. Templates are located under src/main/resources/templates. As you would expect it, changes to template files are immediatly picked up when running Quarkus in Dev Mode. By default, the template name is derived from the field name of the injected Template instance, i.e. in this case the src/main/resources/templates/todos.html template will be used. It could look like this:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;My Todos\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;My Todos\u0026lt;/h1\u0026gt; \u0026lt;table class=\"table table-striped table-bordered\"\u0026gt; \u0026lt;thead\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Id\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" \u0026gt;Title\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Priority\u0026lt;/th\u0026gt; \u0026lt;th scope=\"col\" class=\"fit\"\u0026gt;Completed\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026lt;/thead\u0026gt; {#if todos.size == 0} (1) \u0026lt;tr\u0026gt; \u0026lt;td colspan=\"4\"\u0026gt;No data found.\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {#else} {#for todo in todos} (2) \u0026lt;tr\u0026gt; \u0026lt;th scope=\"row\"\u0026gt;#{todo.id}\u0026lt;/th\u0026gt; \u0026lt;td\u0026gt; {todo.title} (3) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; {todo.priority} (4) \u0026lt;/td\u0026gt; \u0026lt;td\u0026gt; (5) \u0026lt;div class=\"custom-control custom-checkbox\"\u0026gt; \u0026lt;input type=\"checkbox\" class=\"custom-control-input\" disabled id=\"completed-{todo.id}\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"custom-control-label\" for=\"completed-{todo.id}\"\u0026gt;\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; {/for} {/if} \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 If the injected todos list is empty, display a placeholder row   2 Otherwise, iterate over the todos list and add a table row for each one   3 Table cell for title   4 Table cell for priority   5 Table cell for completion status, rendered as a checkbox    If you\u0026#8217;ve worked with other templating engine before, this will look very familiar to you. You can refer to injected objects and their properties to display their values, have conditional logic, iterate over collections etc. A very nice aspect about Qute templates is that they are processed at build time, following the Quarkus notion of \"compile-time boot\". This means if there is an error in a template such as unbalanced control keywords, you\u0026#8217;ll find out about this at build time instead of only at runtime.\n The reference documentation describes the syntax and all options in depth. Note that things are still in flux here, e.g. I couldn\u0026#8217;t work with boolean operators in conditions.\n   Combining HTML and Data APIs Thanks to HTTP content negotiation, you can easily combine resource methods for returning HTML and JSON for API-style consumers in a single endpoint. Just add another resource method for handling the required media type, e.g. \"application/json\":\n 1 2 3 4 5 6 @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson() { return Todo.findAll().list(); }    A standard HTTP request issued by a web browser would now be answered with the HTML page, whereas an AJAX request with the \"application/json\" accept header (or a manual invocation via curl) would yield the JSON representation. I really like that idea of considering HTML and JSON-based representations as two different \"views\" of the same API essentially.\n   Template Organization If a web application has multiple pages or \"views\", chances are there are many similarities between those. E.g. there might be a common header and footer for all pages, or one and the same form is used on multiple pages.\n To avoid duplication in the templates in such cases, Qute supports the notion of includes. E.g. let\u0026#8217;s say there\u0026#8217;s a common form for creating new and editing existing todos. This can be put into its own template:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 (1) \u0026lt;form action=\"/todo/{#if update}{todo.id}/edit{#else}new{/if}\" method=\"POST\" name=\"todoForm\" enctype=\"multipart/form-data\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"title\"\u0026gt;Title\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"title\" class=\"form-control\" id=\"title\" placeholder=\"Title\" required autofocus {#if update}value=\"{todo.title}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;select class=\"custom-select\" name=\"priority\"\u0026gt; \u0026lt;option disabled value=\"\"\u0026gt;Priority\u0026lt;/option\u0026gt; {#for prio in priorities} \u0026lt;option value=\"{prio}\" {#if todo.priority == prio}selected{/if}\u0026gt;{prio}\u0026lt;/option\u0026gt; {/for} \u0026lt;/select\u0026gt; \u0026lt;/div\u0026gt; (3) {#if update} \u0026lt;div class=\"col-auto my-1\"\u0026gt; \u0026lt;div class=\"form-check\"\u0026gt; \u0026lt;input type=\"checkbox\" name=\"completed\" class=\"form-check-input\" id=\"completed\" {#if todo.completed}checked{/if}\u0026gt; \u0026lt;label class=\"form-check-label\" for=\"completed\"\u0026gt;Completed\u0026lt;/label\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; {/if} (4) \u0026lt;button type=\"submit\" class=\"btn btn-primary\"\u0026gt;{#if update}Update{#else}Create{/if}\u0026lt;/button\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Post to different path for update and create   2 Display existing title and priority in case of an update   3 Show checkbox for completion status in case of an update   4 Choose button caption depending on use case    In order to display this form right under the table with all todos, the template can simply be included like so:\n 1 2 \u0026lt;h2\u0026gt;New Todo\u0026lt;/h2\u0026gt; {#include todo-form.html}{/include}    It\u0026#8217;s also possible to extract the outer shell of multiple pages into a shared template (\"template inheritance\"). This allows to extract common headers and footers into one single template with placeholders for the inner parts.\n For that, create a template with the common outer structure:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 \u0026lt;!doctype html\u0026gt; \u0026lt;html lang=\"en\"\u0026gt; \u0026lt;head\u0026gt; \u0026lt;meta charset=\"utf-8\"\u0026gt; \u0026lt;!-- CSS ... --\u0026gt; \u0026lt;link rel=\"stylesheet\" href=\"...\"\u0026gt; \u0026lt;title\u0026gt;{#insert title}Default Title{/}\u0026lt;/title\u0026gt; (1) \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\"container\"\u0026gt; \u0026lt;h1\u0026gt;{#insert title}Default Title{/}\u0026lt;/h1\u0026gt; (1) {#insert contents}No contents!{/} (2) \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt;      1 Derived templates define a section title which will be inserted here   2 Derived templates define a section contents which will be inserted here    Other templates can then extend the base one, e.g. like so for the \"Edit Todo\" page:\n 1 2 3 4 5 6 {#include base.html} (1) {#title}Edit Todo #{todo.id}{/title} (2) {#contents} (3) {#include todo-form.html}{/include} (4) {/contents} {/include}      1 Include the base template   2 Define the title section   3 Define the contents section   4 Include the template for displaying the todo form    As so often, a balance needs to be found between extracting common parts and still being able to comprehend the overall structure without having to pursue a large number of template references. But in any case with includes and inserts Qute puts the neccessary tools into your hands.\n   Error Handling For a great user experience robust error handling is a must. E.g. might happen that a user loads the \"Edit Todo\" dialog and while they\u0026#8217;re in the process of editing, that record gets deleted by someone else. When saving, a proper error message should be displayed to the first user. Here\u0026#8217;s the resource method implementation for that:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @POST @Consumes(MediaType.MULTIPART_FORM_DATA) @Transactional @Path(\"/{id}/edit\") public Object updateTodo( @PathParam(\"id\") long id, @MultipartForm TodoForm todoForm) { Todo loaded = Todo.findById(id); (1) if (loaded == null) { (2) return error.data(\"error\", \"Todo with id \" + id + \" has been deleted after loading this form.\"); } loaded = todoForm.updateTodo(loaded); (3) return Response.status(301) (4) .location(URI.create(\"/todo\")) .build(); }      1 Load the todo record to be updated   2 If it doesn\u0026#8217;t exist, render the \"error\" template   3 Otherwise, update the record; as loaded is an attached entity, no call to persist is needed   4 redirect the user to the main page, avoiding issues with reloading etc. (post-redirect-get pattern)    Note that TemplateInstance as returned from the Template#data() method doesn\u0026#8217;t extend the JAX-RS Response class. Therefore the return type of the method must be declared as Object in this case.\n   Search Thanks to Hibernate Panache it\u0026#8217;s quite simple to refine the todo list and only return those whose title matches a given search term. Also ordering the list in some meaningful way would be nice. All we need is an optional query parameter for specifying the search term and a custom query method:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 @GET @Consumes(MediaType.TEXT_HTML) @Produces(MediaType.TEXT_HTML) public TemplateInstance listTodos(@QueryParam(\"filter\") String filter) { return todos.data(\"todos\", find(filter)); } @GET @Produces(MediaType.APPLICATION_JSON) @Consumes(MediaType.APPLICATION_JSON) public List\u0026lt;Todo\u0026gt; listTodosJson(@QueryParam(\"filter\") String filter) { return find(filter); } private List\u0026lt;Todo\u0026gt; find(String filter) { Sort sort = Sort.ascending(\"completed\") (1) .and(\"priority\", Direction.Descending) .and(\"title\", Direction.Ascending); if (filter != null \u0026amp;\u0026amp; !filter.isEmpty()) { (2) return Todo.find(\"LOWER(title) LIKE LOWER(?1)\", sort, \"%\" + filter + \"%\").list(); } else { return Todo.findAll(sort).list(); (3) } }      1 First sort by completion status, then priority, then by title   2 If a filter is given, apply the search term lower-cased and with wildcards, i.e. using a WHERE clause such as where lower(todo0_.title) like lower(%searchterm%)   3 Otherwise, return all todos    To enter the search term, a form is added next to the table of todos:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; (2) \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; (3) \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 Invoke this page with the entered search as query parameter   2 Input for the search term; show the previously entered term, if any   3 A button for clearing the result list if a search term has been entered; otherwise the button will be disabled      Smoother User Experience via Unpoly The last thing I wanted to explore is how the usability and performance of the application can be improved by means of some client-side enhancements. By default, a web app rendered on the server-side like ours requires full page loads when going from one page to the other. This is where single page applications (SPAs) implemented with client-side frameworks shine: just parts of the document object model tree in the browser will be replaced e.g. when loading a result list via AJAX, resulting in a much smoother and faster user experience.\n Does this mean we have to give up on server-side rendering altogether if we\u0026#8217;re after this kind of UX? Luckily not, as small helper libraries such as Unpoly, Intercooler or Turbolinks can be leveraged to replace just page fragments instead of requiring full page loads. This results in a smooth SPA-like user experience without having to opt into the full client-side programming model. For the Todo example I\u0026#8217;ve obtained great results using Unpoly. After importing its JavaScript file, all that\u0026#8217;s needed is to add the up-target attribute to links or forms.\n E.g. here\u0026#8217;s the form for entering the search term with that modification:\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 (1) \u0026lt;form action=\"/todo\" method=\"GET\" name=\"search\" up-target=\".container\"\u0026gt; \u0026lt;div class=\"form-row align-items-center\"\u0026gt; \u0026lt;div class=\"col-sm-3 my-1\"\u0026gt; \u0026lt;label class=\"sr-only\" for=\"filter\"\u0026gt;Search\u0026lt;/label\u0026gt; \u0026lt;input type=\"text\" name=\"filter\" class=\"form-control\" id=\"filter\" placeholder=\"Search By Title\" required {#if filtered}value=\"{filter}\"{/if}\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;input class=\"btn btn-primary\" value=\"Search\" type=\"submit\"\u0026gt;\u0026amp;nbsp; (2) \u0026lt;a class=\"btn btn-secondary {#if !filtered}disabled{/if}\" href=\"/todo\" role=\"button\" up-target=\".container\"\u0026gt;Clear Filter\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/form\u0026gt;      1 When receiving the result of the form submission, replace the \u0026lt;div\u0026gt; with CSS class container of the current page with the one from the response   2 Do the same when following the \"Clear Filter\" link    The magic trick of Unpoly is that links and forms with the up-target attribute are intercepted by Unpoly and executed via AJAX calls. The specified fragments from the result page are then used to replace parts of the already loaded page, instead of having the browser load the full response page. The result is the fast user experience shown in the video above.\n Unpoly also allows to show page fragments in modal dialogs, allowing to remain on the same page also when showing forms such as the one for editing a todo:\n   Note that if JavaScript is disabled, the application gracefully falls back to full page loads. I.e. it will still be fully functional, just with a slightly degraded user experience. The same would happen when accessing the edit dialog directly via its URL or when opening the \"Edit\" link in a new tab or window:\n     Bonus: Using WebJars In a thread on Twitter James Ward brought up the idea of pulling in required resources such as Bootstrap via WebJars instead of getting them from a CDN. WebJars is a useful utility for obtaining all sorts of client-side libraries with Java build tools such as Maven or Gradle.\n For Bootstrap, the following dependency must be added to the Maven pom.xml file:\n 1 2 3 4 5 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.webjars\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;bootstrap\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;4.4.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;    The Bootstrap CSS can then be included within the base.html template like so:\n 1 2 3 4 5 6 7 ... \u0026lt;head\u0026gt; ... \u0026lt;link rel=\"stylesheet\" href=\"/webjars/bootstrap/4.4.1/css/bootstrap.min.css\"\u0026gt; ... \u0026lt;/head\u0026gt; ...    This is all that\u0026#8217;s needed in order to use Bootstrap via WebJars. Note this will work on the JVM and also with a native binary via GraalVM: WebJars resources are located under META-INF/resources, and Quarkus automatically adds all resources from there when building a native image.\n   Wrap Up This concludes my quick tour through server-side web applications with Quarkus and its new Qute extension. Where only web applications based on REST APIs called by client-side web applications were supported before, Qute is a great addition to the list of Quarkus extensions, allowing to choose different architecture styles based on your needs and preferences.\n Note that Qute currently is in \"Experimental\" state, i.e. it\u0026#8217;s a great time to give it a try and share your feedback, but be prepared for possible immaturities and potential changes down the road. E.g. I noticed that complex boolean expressions in template conditions aren\u0026#8217;t support yet. Also it would be great to get build-time feedback upon invalid variable references in templates.\n To learn more, refer to the Qute guide and its reference documentation. You can find the complete source code of the Todo example including instructions for building and running in this GitHub repo.\n  ","id":11,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eOne of the long-awaited features in Quarkus was support for server-side templating:\nuntil recently, Quarkus supported only client-side web frameworks which obtain there data by calling a REST API on the backend.\nThis has changed with \u003ca href=\"https://quarkus.io/blog/quarkus-1-1-0-final-released/\"\u003eQuarkus 1.1\u003c/a\u003e: it comes with a brand-new template engine named \u003ca href=\"https://quarkus.io/guides/qute\"\u003eQute\u003c/a\u003e,\nwhich allows to build web applications using server-side templates.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Quarkus Qute – A Test Ride","uri":"http://localhost:1313/blog/quarkus-qute-test-ride/","year":"2020"},{"content":"As a software engineer, I like to automate tedious tasks as much as possible. The deployment of this website is no exception: it is built using the Hugo static site generator and hosted on GitHub Pages; so wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\n With the advent of GitHub Actions, tasks like this can easily be implemented without having to rely on any external CI service. Instead, many ready-made actions can be obtained from the GitHub marketplace and easily be configured as per our needs. E.g. triggered by a push to a specified branch in a GitHub repository, they can execute tasks like project builds, tests and many others, running in virtual machines based on Linux, Windows and even macOS. So let\u0026#8217;s see what\u0026#8217;s needed for building a Hugo website and deploying it to GitHub Pages.\n GitHub Actions To the Rescue Using my favourite search engine, I came across two GitHub actions which do everything we need:\n   GitHub Actions for Hugo\n  GitHub Actions for GitHub Pages\n   There are multiple alternatives for GitHub Pages deployment. I chose this one basically because it seems to be the most popular one (as per number of GitHub stars), and because it\u0026#8217;s by the same author as the Hugo one, so they should nicely play together.\n   Registering a Deploy Key In order for the GitHub action to deploy the website, a GitHub deploy key must be registered.\n To do so, create a new SSH key pair on your machine like so:\n ssh-keygen -t rsa -b 4096 -C \"$(git config user.email)\" -f gh-pages -N \"\"   This will create two files, the public key (gh-pages.pub) and the private key (gh-pages). Go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/keys and click \"Add deploy key\". Paste in the public part of your key pair and check the \"Allow write access\" box.\n No go to https://github.com/\u0026lt;your-user-or-organisation\u0026gt;/\u0026lt;your-repo\u0026gt;/settings/secrets and click \"Add new secret\". Choose ACTIONS_DEPLOY_KEY as the name and paste the private part of your key pair into the \"Value\" field.\n The key will be stored in an encrypted way as per GitHub\u0026#8217;s documentation Nevertheless I\u0026#8217;d recommend to use a specific key pair just for this purpose, instead of re-using any other key pair. That way, impact will be reduced to this particular usage, should the private key get leaked somehow.\n   Defining the Workflow With the key in place, it\u0026#8217;s time to set up the actual GitHub Actions workflow. This is simply done by creating the file .github/workflows/gh-pages-deployment.yml in your repository with the following contents. GitHub Actions workflows are YAML files, because YOLO ;)\n 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 name: GitHub Pages on: (1) push: branches: - master jobs: build-deploy: runs-on: ubuntu-18.04 steps: - uses: actions/checkout@v1 (2) with: submodules: true - name: Install Ruby Dev (3) run: sudo apt-get install ruby-dev - name: Install AsciiDoctor and Rouge run: sudo gem install asciidoctor rouge - name: Setup Hugo (4) uses: peaceiris/actions-hugo@v2 with: hugo-version: '0.62.0' - name: Build (5) run: hugo - name: Deploy (6) uses: peaceiris/actions-gh-pages@v2 env: ACTIONS_DEPLOY_KEY: ${{ secrets.ACTIONS_DEPLOY_KEY }} PUBLISH_BRANCH: gh-pages PUBLISH_DIR: ./public      1 Run this action whenever changes are pushed to the master branch   2 The first step in the job: check out the source code   3 Install AsciiDoctor (in case you use Hugo with AsciiDoc files, like I do) and Rouge, a Ruby gem for syntax highlighting; I\u0026#8217;m installing the gems instead of Ubuntu packages in order to get current versions   4 Set up Hugo via the aforementioned GitHub Actions for Hugo   5 Run the hugo command; here you could add parameters such as -F for also building future posts   6 Deploy the website to GitHub pages; the contents of Hugo\u0026#8217;s build directory public will be pushed to the gh-pages branch of the upstream repository, using the deploy key configured before    And that\u0026#8217;s all we need; once the file is committed and pushed to the upstream repository, the deployment workflow will be executed upon each push to the master branch.\n You can find the complete workflow definition used for publishing this website here. Also check out the documentation of GitHub Actions for Hugo and GitHub Actions for GitHub Pages to learn more about their capabilities and the options they offer.\n  ","id":12,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eAs a software engineer, I like to automate tedious tasks as much as possible.\nThe deployment of this website is no exception:\nit is built using the \u003ca href=\"https://gohugo.io/\"\u003eHugo\u003c/a\u003e static site generator and hosted on \u003ca href=\"https://pages.github.com/\"\u003eGitHub Pages\u003c/a\u003e;\nso wouldn\u0026#8217;t it be nice if the rendered website would automatically be published whenever an update is pushed to its source code repository?\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Automatically Deploying a Hugo Website via GitHub Actions","uri":"http://localhost:1313/blog/automatically-deploying-hugo-website-via-github-actions/","year":"2019"},{"content":"It has been quite a while since the last post on my old personal blog; since then, I\u0026#8217;ve mostly focused on writing about my day-work on the Debezium blog as well as some posts about more general technical topics on the Hibernate team blog.\n Now recently I had some ideas for things I wanted to write about, which didn\u0026#8217;t feel like a good fit for neither of those two. So it was time to re-boot a personal blog. The previous Blogger based one really, really feels outdated by now. Plus, I also wanted to have more control over how things work, and also be able to publish a list of projects I work on, conference talks I gave etc. So I decided to build the site using Hugo, a static site generator, and also use a nice new shiny dev domain. And here we are, welcome to morling.dev!\n Stay tuned for more posts every now and then about anything related to open source, the projects I work on and software engineering in general. Onwards!\n","id":13,"section":"blog","summary":"\u003cdiv class=\"paragraph\"\u003e\n\u003cp\u003eIt has been quite a while since the last post on my old \u003ca href=\"http://musingsofaprogrammingaddict.blogspot.com/\"\u003epersonal blog\u003c/a\u003e;\nsince then, I\u0026#8217;ve mostly focused on writing about my day-work on the \u003ca href=\"https://debezium.io/blog/\"\u003eDebezium blog\u003c/a\u003e as well as \u003ca href=\"https://in.relation.to/gunnar-morling/\"\u003esome posts\u003c/a\u003e about more general technical topics on the Hibernate team blog.\u003c/p\u003e\n\u003c/div\u003e","tags":null,"title":"Time for a New Blog","uri":"http://localhost:1313/blog/time-for-new-blog/","year":"2019"}]